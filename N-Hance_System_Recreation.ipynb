{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Names: Jorge Mazariegos & Cameron Knopp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/camknopp/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# imports statements\n",
    "import time\n",
    "from collections import defaultdict, OrderedDict, Counter\n",
    "import string\n",
    "import itertools\n",
    "import operator\n",
    "from scipy.stats import iqr\n",
    "from statistics import median\n",
    "import math\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "#from gensim.models import KeyedVectors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "set(stopwords.words('english'))\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-paper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess should take in the datasets (.xml) and prepare them to be used\n",
    "def preprocess(data):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data (list(str)):\n",
    "    Returns: a list of tokens and a list of tokenized sentences\n",
    "\n",
    "    \"\"\"\n",
    "    #######################################################\n",
    "    # initialize variables to use in preprocess\n",
    "    #######################################################\n",
    "    puns = []\n",
    "    tokens = []\n",
    "    stop = stopwords.words('english')\n",
    "    \n",
    "    #######################################################\n",
    "    # Given a sentence, tokenize it and append it to a list\n",
    "    #######################################################\n",
    "    for sentence in data:\n",
    "        puns.append(word_tokenize(sentence.lower())) # creates the list of all sentences\n",
    "        \n",
    "    #######################################################\n",
    "    # Every sentence is tokenized, but let's grab each\n",
    "    # individual word to make a vocab out of.\n",
    "    #######################################################\n",
    "    for sentence in puns:\n",
    "        for word in sentence:\n",
    "            if(word.isalpha()): # filter out punctuation\n",
    "                tokens.append(word)\n",
    "    #######################################################\n",
    "    # Remove stop words from tokens\n",
    "    #######################################################\n",
    "    tokens_with_stop_words = tokens\n",
    "    tokens = [token for token in tokens_with_stop_words if token not in stop]\n",
    "\n",
    "    return tokens, puns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA PROCESSING #\n",
    "#######################################################\n",
    "# Open the dataset/'s we will be using and process the\n",
    "# text within to be used by our code.\n",
    "#######################################################\n",
    "f = open('datasets/data/test/subtask1-heterographic-test.xml', 'r', encoding = 'utf8')\n",
    "data = f.read()\n",
    "\n",
    "#######################################################\n",
    "# Using Beautiful Soup we can easily extract the puns\n",
    "# from the given datasets.\n",
    "#######################################################\n",
    "soup = BeautifulSoup(data, 'xml')\n",
    "ids = soup.find_all('text')\n",
    "words = soup.find_all('word')\n",
    "#######################################################\n",
    "# Create a list of all puns within the dataset to hand\n",
    "# over to our preprocess function\n",
    "#######################################################\n",
    "wurd = \"\"\n",
    "sentence = \"\"\n",
    "sentences = []\n",
    "punList = []\n",
    "for i in range(0, len(ids)):\n",
    "    for line in ids[i]:\n",
    "        for word in line:\n",
    "            if(word != '\\n' or word == '\\''):\n",
    "                if(word.isalpha()): # If not punctuation\n",
    "                    wurd = word\n",
    "                    if(sentence == \"\"): # If the start of the sentence\n",
    "                        sentence = sentence + wurd\n",
    "                    else: # If not the start of the sentence\n",
    "                        sentence = sentence + \" \" + wurd\n",
    "                else: # If punctuation we don't want to put a space between the character and it.\n",
    "                    wurd = word\n",
    "                    sentence = sentence + wurd\n",
    "                wurd = \"\" # clear the current word\n",
    "    sentences.append(sentence) # append the created string sentence to our list.\n",
    "    sentence = \"\"\n",
    "#######################################################\n",
    "# Create a list of tokens to make a vocabulary of and\n",
    "# create a list of sentences to create make word pairs\n",
    "# from.\n",
    "#######################################################\n",
    "tokens, punList = preprocess(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, special_tokens=None):\n",
    "        self.w2idx = {}\n",
    "        self.idx2w = {}\n",
    "        self.w2cnt = defaultdict(int)\n",
    "        self.special_tokens = special_tokens\n",
    "        if self.special_tokens is not None:\n",
    "            self.add_tokens(special_tokens)\n",
    "\n",
    "    def add_tokens(self, tokens):\n",
    "        for token in tokens:\n",
    "            self.add_token(token)\n",
    "            self.w2cnt[token] += 1\n",
    "\n",
    "    def add_token(self, token):\n",
    "        if token not in self.w2idx:\n",
    "            cur_len = len(self)\n",
    "            self.w2idx[token] = cur_len\n",
    "            self.idx2w[cur_len] = token\n",
    "\n",
    "    def prune(self, min_cnt=2):\n",
    "        to_remove = set([token for token in self.w2idx if self.w2cnt[token] < min_cnt])\n",
    "        if self.special_tokens is not None:\n",
    "            to_remove = to_remove.difference(set(self.special_tokens))\n",
    "        \n",
    "        for token in to_remove:\n",
    "            self.w2cnt.pop(token)\n",
    "            \n",
    "        self.w2idx = {token: idx for idx, token in enumerate(self.w2cnt.keys())}\n",
    "        self.idx2w = {idx: token for token, idx in self.w2idx.items()}\n",
    "    \n",
    "    def __contains__(self, item):\n",
    "        return item in self.w2idx\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        if isinstance(item, str):\n",
    "            return self.w2idx[item]\n",
    "        elif isinstance(item , int):\n",
    "            return self.idx2w[item]\n",
    "        else:\n",
    "            raise TypeError(\"Supported indices are int and str\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return(len(self.w2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "# Using skipgrams we can create the wordpairs described\n",
    "# in the N-Hance research paper.\n",
    "#######################################################\n",
    "\n",
    "class SkipGramDataset(Dataset):\n",
    "    def __init__(self, data, vocab, skip_window=3):\n",
    "        super().__init__()\n",
    "\n",
    "        #######################################################\n",
    "        # Unlike before, data will be a list of strings handed\n",
    "        # all at once.\n",
    "        #######################################################\n",
    "        self.vocab = vocab\n",
    "        self.data = data\n",
    "        self.skip_window = skip_window\n",
    "        self.pairs = self._generate_pairs(data, skip_window)\n",
    "        \n",
    "    #######################################################\n",
    "    #\n",
    "    #######################################################\n",
    "    def _generate_pairs(self, data, skip_window):\n",
    "        \"\"\"\n",
    "        Args: input data (a list of lists of words for each sentence (i.e, each list of words is a sentence))\n",
    "        Returns: a list of lists. Each list will contain the word pairs for a given sentence in the input dataset\n",
    "        \"\"\"\n",
    "        pairs = [[]]  # list of word pairs for each sentence\n",
    "        curr_sentence_pairs = [] # list of word pairs for current sentence\n",
    "        pruned_pairs = []\n",
    "        \n",
    "        for sent in data: \n",
    "            for i in range(len(sent)):\n",
    "                for j in range(-skip_window, skip_window + 1):\n",
    "                    context_idx = i + j\n",
    "                    if j == 0 or context_idx < 0 or context_idx >= len(sent):\n",
    "                        continue\n",
    "                    if sent[i] not in self.vocab or sent[context_idx] not in self.vocab:\n",
    "                        continue\n",
    "                    curr_sentence_pairs.append((sent[i], sent[context_idx]))\n",
    "                    \n",
    "            \"\"\"\n",
    "            for (a, b) in list(curr_sentence_pairs):\n",
    "                print(\"({}, {}) exists in cur_sentence_pairs\".format(a,b))\n",
    "                if (b, a) in curr_sentence_pairs:\n",
    "                    print(\"deleting ({}, {})\".format(b, a))\n",
    "                    del (b, a)\n",
    "            \"\"\"\n",
    "            \n",
    "            pairs.append(curr_sentence_pairs.copy()) # need to append a copy so that it is not cleared with we call clear() in the next line\n",
    "            curr_sentence_pairs.clear()\n",
    "            #print(pairs[len(pairs)-1])\n",
    "                    \n",
    "        return pairs\n",
    "    \n",
    "    #######################################################\n",
    "    #\n",
    "    #######################################################\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        pair = self.pairs[idx]\n",
    "        print(pair)\n",
    "        #print(pair)\n",
    "        #pair = [self.vocab[t] for t in pair]\n",
    "        #pair = [self.vocab.__getitem__(t) for t in pair]\n",
    "        return pair\n",
    "    \n",
    "    #######################################################\n",
    "    #\n",
    "    #######################################################\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        \"\"\"\n",
    "        return len(self.pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "# Create our Vocabulary\n",
    "#######################################################\n",
    "voc = Vocabulary()\n",
    "voc.add_tokens(tokens)\n",
    "vocab_size = len(voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_puns(skipgram):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        list of lists of list of words representing the input sentences\n",
    "    output:\n",
    "        list of boolean values, one boolean value for each input sentence.\n",
    "        True = yes, the sentence did contain a pun\n",
    "        False = no, the sentence did not contain a pun\n",
    "    \"\"\"\n",
    "    # calculate the pmi scores between the two words in each pair & store this in a dictionary with the index...\n",
    "    # ... being the word pair and the value being the pmi score\n",
    "    # generate threshold value for each sentence of pairs\n",
    "    # if any pmi score is above this threshold value for a given sentence, then that sentence contains a pun\n",
    "    # generate bigrams for each sentence in order to determine PMI scores (since we need to know the probability of a word)\n",
    "   \n",
    "    # get count of instances of bigram in all sentences (instances of this word pair / total word pairs)\n",
    "    # divide by count of first word/ total number of words *count of second word/ total num words\n",
    "    \n",
    "    \n",
    "    # create a Counter object to get counts of individual words\n",
    "    all_sentences = list(itertools.chain.from_iterable(skipgram.data.copy())) \n",
    "    word_counts = Counter(all_sentences)\n",
    "    total_words = len(all_sentences)\n",
    "    \n",
    "    # get list of lists of word_pairs for each sentence\n",
    "    word_pairs = skipgram.pairs.copy()\n",
    "    word_pairs = [[(a,b) for (a,b) in sent] for sent in word_pairs] \n",
    "        \n",
    "    # create Counter object to get counts for each word_pair\n",
    "    all_word_pairs= list(itertools.chain.from_iterable(word_pairs.copy())) # join all sentences together\n",
    "    all_word_pairs = [(a,b) for (a,b) in all_word_pairs] \n",
    "    total_word_pairs = len(all_word_pairs)\n",
    "    word_pair_count = Counter(all_word_pairs)\n",
    "    \n",
    "    # create a list of dictionaries for each sentence { word_pair : pmi_score }\n",
    "    pmi_scores = list(dict())\n",
    "    current_pmi = 0\n",
    "    current_dict = {}\n",
    "    \n",
    "    # now we will calculate the PMI score for each word_pair\n",
    "    # the formula for PMI score is: log[p(x,y) / (p(x)*p(y))]\n",
    "    \n",
    "    for i in range(skipgram.__len__()):\n",
    "        \n",
    "        # for each sentence, find pmi score for each individual word_pair\n",
    "        for w_pair in word_pairs[i]:\n",
    "            numerator = word_pair_count[w_pair] / total_word_pairs\n",
    "            denominator = (word_counts[w_pair[0]] / total_words) * (word_counts[w_pair[1]] / total_words)\n",
    "            current_pmi =  numerator / denominator\n",
    "            current_pmi = math.log(current_pmi)\n",
    "        \n",
    "            current_dict.update({w_pair : current_pmi}) # add bigram's pmi score to dictionary at index i (the current sentence)\n",
    "        \n",
    "        pmi_scores.append(current_dict.copy())\n",
    "        current_dict.clear()\n",
    "        \n",
    "    \n",
    "    # now we sort the dictionary entries from highest->lowest based on value (PMI score)\n",
    "    ordered_pmi_scores = list(OrderedDict())\n",
    "    \n",
    "    for i in range(len(pmi_scores)):\n",
    "        current_dict = pmi_scores[i]\n",
    "        # convert to dictionary ordered by value (which is the pmi score in this case)\n",
    "        current_ordered_dict = OrderedDict(sorted(current_dict.items(), key=lambda x: x[1], reverse=True))\n",
    "        ordered_pmi_scores.append(current_ordered_dict.copy())\n",
    "        current_ordered_dict.clear()\n",
    "    \n",
    "    # now we need to find the interquartile range for each dictionary\n",
    "    # this is found using the iqr function from scipy.stats\n",
    "    iqr_values = []\n",
    "    \n",
    "    for dictionary in ordered_pmi_scores:\n",
    "        iqr_values.append(iqr(list(dictionary.values())))\n",
    "    \n",
    "    \n",
    "    # now we take the median of these iqr values and take that as our iqr value of the current dataset\n",
    "    median_iqr = median(iqr_values)\n",
    "    print(median_iqr)\n",
    "\n",
    "    # create a list which will contain True (yes, this sentence contains a pun) or False (no, this sentence does not contain a pun)\n",
    "    # ... at each index\n",
    "    contains_pun = []\n",
    "     \n",
    "    for dictionary in ordered_pmi_scores:\n",
    "        curr_dict = list(dictionary.items())\n",
    "        \n",
    "        if len(curr_dict) > 1:\n",
    "            if curr_dict[0][1] - curr_dict[1][1] > median_iqr:\n",
    "                contains_pun.append(True)\n",
    "            else:\n",
    "                contains_pun.append(False)\n",
    "        else:\n",
    "            contains_pun.append(False)\n",
    "            \n",
    "    #print(contains_pun)\n",
    "    \n",
    "    \n",
    "    \n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11778303565638382\n"
     ]
    }
   ],
   "source": [
    "#######################################################\n",
    "# Create our Dataset and call detect_puns (subtask 1)\n",
    "#######################################################\n",
    "\n",
    "# need to adjust skip_window because we want each word to be paired w/ every word in the sentence, not just 2 to the left & 2 to the right\n",
    "dataset = SkipGramDataset(punList, voc, skip_window=2)\n",
    "detect_puns(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class SkipGramModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size (int): vocabulary size\n",
    "            embedding_dim (int): the dimension of word embeddings\n",
    "        \"\"\"\n",
    "        ### INSERT YOUR CODE BELOW ###\n",
    "        #self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        #self.linear = torch.nn.Linear(1, vocab_size)\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = torch.nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "        ### INSERT YOUR CODE ABOVE ###\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Perform the forward pass of the skip-gram model.\n",
    "        \n",
    "        Args:\n",
    "            inputs (torch.LongTensor): input tensor containing batches of word ids [Bx1]\n",
    "        Returns:\n",
    "            outputs (torch.FloatTensor): output tensor with unnormalized probabilities over the vocabulary [BxV]\n",
    "        \"\"\"\n",
    "        ### INSERT YOUR CODE BELOW ###\n",
    "        embeds = self.embedding(inputs)\n",
    "        #embeds = self.embedding(inputs)\n",
    "        outputs = self.linear(embeds)\n",
    "        outputs=outputs\n",
    "        #output = F.log_softmax(self.linear(embeds), dim=1)\n",
    "        ### INSERT YOUR CODE ABOVE ###\n",
    "        return outputs\n",
    "    \n",
    "    def save_embeddings(self, voc, path):\n",
    "        \"\"\"\n",
    "        Save the embedding matrix to a specified path.\n",
    "        \n",
    "        Args:\n",
    "            voc (Vocabulary): the Vocabulary object for id-to-token mapping\n",
    "            path (str): the location of the target file\n",
    "        \"\"\"\n",
    "        ### INSERT YOUR CODE BELOW ###\n",
    "        embeds = self.embedding.weight.data.cpu().numpy()\n",
    "        f = open(path, 'w')\n",
    "        f.write(str(vocab_size) + ' ' + str(embedding_dim) + '\\n')\n",
    "        \n",
    "        for idx in range(len(embeds)):\n",
    "            word = voc.idx2w[idx]\n",
    "            embedding = ' '.join(map(str,embeds[idx]))\n",
    "            f.write(word + ' '+ embedding + '\\n')\n",
    "        ### INSERT YOUR CODE ABOVE ###\n",
    "        print(\"Successfuly saved to {}\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
