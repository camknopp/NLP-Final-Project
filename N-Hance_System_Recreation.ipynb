{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Names: Jorge Mazariegos & Cameron Knopp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/camknopp/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# imports statements\n",
    "import time\n",
    "import string\n",
    "import itertools\n",
    "import operator\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import nltk\n",
    "import re\n",
    "from scipy.stats import iqr\n",
    "from statistics import median\n",
    "from collections import defaultdict, OrderedDict, Counter\n",
    "from bs4 import BeautifulSoup\n",
    "#from gensim.models import KeyedVectors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "set(stopwords.words('english'))\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-paper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess should take in the datasets (.xml) and prepare them to be used\n",
    "def preprocess(data):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sentences (list(str)):\n",
    "    Returns: a list of tokens and a list of tokenized sentences\n",
    "\n",
    "    \"\"\"\n",
    "    #######################################################\n",
    "    # initialize variables to use in preprocess\n",
    "    #######################################################\n",
    "    puns = []\n",
    "    tokens = []\n",
    "    stop = stopwords.words('english') + list(string.punctuation)\n",
    "        \n",
    "    #######################################################\n",
    "    # Given a sentence, tokenize it and append it to a list\n",
    "    #######################################################\n",
    "    for sentence in data:\n",
    "        puns.append(word_tokenize(sentence.lower())) # creates the list of all sentences\n",
    "        \n",
    "    #######################################################\n",
    "    # Every sentence is tokenized, but let's grab each\n",
    "    # individual word to make a vocab out of.\n",
    "    #######################################################\n",
    "    for sentence in puns:\n",
    "        for word in sentence:\n",
    "            if(word.isalpha()): # filter out punctuation\n",
    "                tokens.append(word)\n",
    "    #######################################################\n",
    "    # Remove stop words from tokens\n",
    "    #######################################################\n",
    "    tokens_with_stop_words = tokens\n",
    "    tokens = [token for token in tokens_with_stop_words if token not in stop]\n",
    "\n",
    "    return tokens, puns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(file):\n",
    "\n",
    "    # DATA PROCESSING #\n",
    "    #######################################################\n",
    "    # Open the dataset/'s we will be using and process the\n",
    "    # text within to be used by our code.\n",
    "    #######################################################\n",
    "    #f = open('datasets/data/test/subtask1-heterographic-test.xml', 'r', encoding = 'utf8')\n",
    "    \n",
    "    f = open(file, 'r', encoding = 'utf8')\n",
    "    data = f.read()\n",
    "\n",
    "    #######################################################\n",
    "    # Using Beautiful Soup we can easily extract the puns\n",
    "    # from the given datasets.\n",
    "    #######################################################\n",
    "    soup = BeautifulSoup(data, 'xml')\n",
    "    ids = soup.find_all('text')\n",
    "    words = soup.find_all('word')\n",
    "    #######################################################\n",
    "    # Create a list of all sentences within the dataset to hand\n",
    "    # over to our preprocess function\n",
    "    #######################################################\n",
    "    wurd = \"\"\n",
    "    sentence = \"\"\n",
    "    sentences = []\n",
    "    pun_list = []\n",
    "    \n",
    "    #this will be a dictionary of {sentence : sentence_id}\n",
    "    sentences_dict = dict()\n",
    "    \n",
    "    \n",
    "    for i in range(len(ids)):\n",
    "        #print(ids[i]['id'])\n",
    "        for line in ids[i]:\n",
    "            for word in line:\n",
    "                if(word != '\\n' or word == '\\''):\n",
    "                    if(word.isalpha()): # If not punctuation\n",
    "                        wurd = word\n",
    "                        if(sentence == \"\"): # If the start of the sentence\n",
    "                            sentence = sentence + wurd\n",
    "                        else: # If not the start of the sentence\n",
    "                            sentence = sentence + \" \" + wurd\n",
    "                    else: # If punctuation we don't want to put a space between the character and it.\n",
    "                        wurd = word\n",
    "                        sentence = sentence + wurd\n",
    "                    wurd = \"\" # clear the current word\n",
    "        sentences.append(sentence) # append the created string sentence to our list.\n",
    "        \n",
    "        sentences_dict.update({sentence : ids[i]['id']}) # map the sentence to it's sentence id in the .xml file\n",
    "        sentence = \"\"\n",
    "    #######################################################\n",
    "    # Create a list of tokens to make a vocabulary of and\n",
    "    # create a list of sentences to create make word pairs\n",
    "    # from.\n",
    "    #######################################################\n",
    "    \n",
    "    \n",
    "    tokens, pun_list = preprocess(sentences)\n",
    "    return tokens, pun_list, sentences_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, special_tokens=None):\n",
    "        self.w2idx = {}\n",
    "        self.idx2w = {}\n",
    "        self.w2cnt = defaultdict(int)\n",
    "        self.special_tokens = special_tokens\n",
    "        if self.special_tokens is not None:\n",
    "            self.add_tokens(special_tokens)\n",
    "\n",
    "    def add_tokens(self, tokens):\n",
    "        for token in tokens:\n",
    "            self.add_token(token)\n",
    "            self.w2cnt[token] += 1\n",
    "\n",
    "    def add_token(self, token):\n",
    "        if token not in self.w2idx:\n",
    "            cur_len = len(self)\n",
    "            self.w2idx[token] = cur_len\n",
    "            self.idx2w[cur_len] = token\n",
    "\n",
    "    def prune(self, min_cnt=2):\n",
    "        to_remove = set([token for token in self.w2idx if self.w2cnt[token] < min_cnt])\n",
    "        if self.special_tokens is not None:\n",
    "            to_remove = to_remove.difference(set(self.special_tokens))\n",
    "        \n",
    "        for token in to_remove:\n",
    "            self.w2cnt.pop(token)\n",
    "            \n",
    "        self.w2idx = {token: idx for idx, token in enumerate(self.w2cnt.keys())}\n",
    "        self.idx2w = {idx: token for token, idx in self.w2idx.items()}\n",
    "    \n",
    "    def __contains__(self, item):\n",
    "        return item in self.w2idx\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        if isinstance(item, str):\n",
    "            return self.w2idx[item]\n",
    "        elif isinstance(item , int):\n",
    "            return self.idx2w[item]\n",
    "        else:\n",
    "            raise TypeError(\"Supported indices are int and str\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return(len(self.w2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "# Using skipgrams we can create the wordpairs described\n",
    "# in the N-Hance research paper.\n",
    "#######################################################\n",
    "\n",
    "class SkipGramDataset(Dataset):\n",
    "    def __init__(self, data, vocab, skip_window=3):\n",
    "        super().__init__()\n",
    "\n",
    "        #######################################################\n",
    "        # Unlike before, data will be a list of strings handed\n",
    "        # all at once.\n",
    "        #######################################################\n",
    "        self.vocab = vocab\n",
    "        self.data = data\n",
    "        # set skip_window to the length of the longest sentence in the data set\n",
    "        self.skip_window =  max(data, key=len)\n",
    "        self.pairs = self._generate_pairs(data, skip_window)\n",
    "        \n",
    "        \n",
    "    #######################################################\n",
    "    # generate word pairs given list of lists of words representing each sentence\n",
    "    #######################################################\n",
    "    def _generate_pairs(self, data, skip_window):\n",
    "\n",
    "        pairs = [[]]  # list of word pairs for each sentence\n",
    "        curr_sentence_pairs = [] # list of word pairs for current sentence\n",
    "        pruned_pairs = []\n",
    "        \n",
    "\n",
    "        for sent in data: \n",
    "            for i in range(len(sent)):\n",
    "                for j in range(-skip_window, skip_window + 1):\n",
    "                    context_idx = i + j\n",
    "                    if j == 0 or context_idx < 0 or context_idx >= len(sent):\n",
    "                        continue\n",
    "                    if sent[i] not in self.vocab or sent[context_idx] not in self.vocab:\n",
    "                        continue\n",
    "                        \n",
    "                    # only add in this sentence if the reverse does not already exist in the list\n",
    "                    if (sent[context_idx], sent[i]) not in curr_sentence_pairs:\n",
    "                        curr_sentence_pairs.append((sent[i], sent[context_idx]))\n",
    "                    \n",
    "            pairs.append(curr_sentence_pairs.copy()) # need to append a copy so that it is not cleared with we call clear() in the next line\n",
    "            curr_sentence_pairs.clear()\n",
    "                    \n",
    "        return pairs\n",
    "    \n",
    "    #######################################################\n",
    "    # returns the list of word_pairs for the sentence at the given index\n",
    "    #######################################################\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        pair = self.pairs[idx]\n",
    "\n",
    "        #pair = [self.vocab[t] for t in pair]\n",
    "        #pair = [self.vocab.__getitem__(t) for t in pair]\n",
    "        return pair\n",
    "    \n",
    "    #######################################################\n",
    "    # returns the number of sentences\n",
    "    #######################################################\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pmi_scores(file):\n",
    "    \"\"\"\n",
    "    returns a list of dictionaries (one for each sentence) of {word_pair : pmi_score}\n",
    "    each dictionary is ordered from highest to lowest pmi score\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tokenize dataset and Create a Vocabulary using the tokens\n",
    "    tokens, pun_list, sentences_dict = data_process(file)\n",
    "    voc = Vocabulary()\n",
    "    voc.add_tokens(tokens)\n",
    "    \n",
    "    # create skipgram model using vocab and puns\n",
    "    skipgram = SkipGramDataset(pun_list, voc, skip_window=2)\n",
    "    \n",
    "    # create a Counter object to get counts of individual words\n",
    "    all_sentences = list(itertools.chain.from_iterable(skipgram.data.copy())) \n",
    "    word_counts = Counter(all_sentences)\n",
    "    total_words = len(all_sentences)\n",
    "    \n",
    "    # get list of lists of word_pairs for each sentence\n",
    "    word_pairs = skipgram.pairs.copy()\n",
    "    word_pairs = [[(a,b) for (a,b) in sent] for sent in word_pairs] \n",
    "   # print(word_pairs)\n",
    "        \n",
    "    # create Counter object to get counts for each word_pair\n",
    "    all_word_pairs= list(itertools.chain.from_iterable(word_pairs.copy())) # join all sentences together\n",
    "    all_word_pairs = [(a,b) for (a,b) in all_word_pairs] \n",
    "    total_word_pairs = len(all_word_pairs)\n",
    "    word_pair_count = Counter(all_word_pairs)\n",
    "    \n",
    "    # create a list of dictionaries for each sentence { word_pair : pmi_score }\n",
    "    pmi_scores = list(dict())\n",
    "    current_pmi = 0\n",
    "    current_dict = {}\n",
    "    \n",
    "    # now we will calculate the PMI score for each word_pair\n",
    "    # the formula for PMI score is: log[p(x,y) / (p(x)*p(y))]\n",
    "    \n",
    "    for i in range(skipgram.__len__()):\n",
    "        \n",
    "        # for each sentence, find pmi score for each individual word_pair\n",
    "        for w_pair in word_pairs[i]:\n",
    "            numerator = word_pair_count[w_pair] / total_word_pairs\n",
    "            denominator = (word_counts[w_pair[0]] / total_words) * (word_counts[w_pair[1]] / total_words)\n",
    "            current_pmi =  numerator / denominator\n",
    "            current_pmi = math.log(current_pmi)\n",
    "        \n",
    "            current_dict.update({w_pair : current_pmi}) # add bigram's pmi score to dictionary at index i (the current sentence)\n",
    "        \n",
    "        pmi_scores.append(current_dict.copy())\n",
    "        current_dict.clear()\n",
    "        \n",
    "    \n",
    "    # now we sort the dictionary entries from highest->lowest based on value (PMI score)\n",
    "    ordered_pmi_scores = list(OrderedDict())\n",
    "    \n",
    "    for i in range(len(pmi_scores)):\n",
    "        current_dict = pmi_scores[i]\n",
    "        # convert to dictionary ordered by value (which is the pmi score in this case)\n",
    "        current_ordered_dict = OrderedDict(sorted(current_dict.items(), key=lambda x: x[1], reverse=True))\n",
    "        ordered_pmi_scores.append(current_ordered_dict.copy())\n",
    "        current_ordered_dict.clear()\n",
    "   \n",
    "    #print(ordered_pmi_scores)\n",
    "    return ordered_pmi_scores, skipgram, sentences_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_puns(file, heterographic):\n",
    "    \"\"\"\n",
    "    create word_pairs for sentences in given file\n",
    "    calculate pmi scores for all given word_pairs\n",
    "    calculate the interquartile range for the pmi scores of word_pairs in each sentence\n",
    "    find the median value of the interquartile ranges across all sentences in the given dataset\n",
    "    for each sentence, if the highest pmi score - second highest pmi score > median interquartile range ...\n",
    "    (cont.) then that means that that sentence contains a pun\n",
    "    \"\"\"\n",
    "    \n",
    "    # homographic pun 5 would be referred to as hom5 in the final list (this is based on the N-Hance system's guidelines)\n",
    "    if heterographic:\n",
    "        prefix = \"het\"\n",
    "    else:\n",
    "        prefix = \"hom\"\n",
    "    \n",
    "    # get pmi scores for all word_pairs in the file\n",
    "    ordered_pmi_scores, skipgram = generate_pmi_scores(file)[:2]\n",
    "    \n",
    "    # now we need to find the interquartile range for each dictionary in the list using iqr from scipy.stats\n",
    "    iqr_values = []\n",
    "    \n",
    "    for dictionary in ordered_pmi_scores:\n",
    "        iqr_values.append(iqr(list(dictionary.values())))\n",
    "    \n",
    "    \n",
    "    # now we take the median of these iqr values and take that as our iqr value of the current dataset\n",
    "    median_iqr = median(iqr_values)\n",
    "    \n",
    "    # create a list which will contain True (yes, this sentence contains a pun) or False (no, this sentence does not contain a pun)\n",
    "    # ... at each index\n",
    "    contains_pun = []\n",
    "     \n",
    "    for i in range(1, len(ordered_pmi_scores)):\n",
    "        curr_dict = list(ordered_pmi_scores[i].items())\n",
    "\n",
    "        if len(curr_dict) > 1 :\n",
    "            # if the difference between the highest pmi score and second highest pmi score (cont.)\n",
    "            #... is greater than the median iqr, then the sentence contains a pun\n",
    "            if float(curr_dict[0][1] - curr_dict[1][1]) > median_iqr:\n",
    "                contains_pun.append(prefix + \"_\" + str(i) + \" 1\" )                \n",
    "            else:\n",
    "                contains_pun.append(prefix + \"_\" + str(i) + \" 0\" )\n",
    "        else:\n",
    "            contains_pun.append(prefix + \"_\" + str(i) + \" 0\" )        \n",
    "\n",
    "    # returning more than just one value because these other values are needed in subtask 2 later on\n",
    "    return contains_pun, ordered_pmi_scores, skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Completes subtask 1 (pun detection)\n",
    "Outputs the results for heterographic and homographic puns to two seperate files\n",
    "\n",
    "In order to run the scoring system for the heterographic puns:\n",
    "open terminal and change directory to ~/datasets/scoring/bin\n",
    "then, type (your file paths will vary):\n",
    "java de.tudarmstadt.ukp.semeval2017.task7.scorer.PunScorer -d ~/Desktop/GitHub/NLP-Final-Project/datasets/data/test/subtask1-heterographic-test.gold ~/Desktop/GitHub/NLP-Final-Project/system_output/subtask1_heterographic.txt ~/Desktop/GitHub/NLP-Final-Project/scorer_results/subtask1_heterographic_results.txt\n",
    "\n",
    "for homographic puns:\n",
    "open terminal and change directory to ~/datasets/scoring/bin\n",
    "then, type (your file paths will vary): \n",
    "java de.tudarmstadt.ukp.semeval2017.task7.scorer.PunScorer -d ~/Desktop/GitHub/NLP-Final-Project/datasets/data/test/subtask1-homographic-test.gold ~/Desktop/GitHub/NLP-Final-Project/system_output/subtask1_homographic.txt ~/Desktop/GitHub/NLP-Final-Project/scorer_results/subtask1_homographic_results.txt\n",
    "\n",
    "\"\"\"\n",
    "def subtask1():\n",
    "    contains_pun_heterographic = detect_puns('datasets/data/test/subtask1-heterographic-test.xml', True)[0]\n",
    "    with open('system_output/subtask1_heterographic.txt', 'w') as filehandle:\n",
    "        for pun_result in contains_pun_heterographic:\n",
    "            filehandle.write('{}\\n'.format(pun_result))\n",
    "\n",
    "    contains_pun_homographic = detect_puns('datasets/data/test/subtask1-homographic-test.xml', False)[0]\n",
    "    with open('system_output/subtask1_homographic.txt', 'w') as filehandle:\n",
    "        for pun_result in contains_pun_homographic:\n",
    "            filehandle.write('{}\\n'.format(pun_result))\n",
    "            \n",
    "subtask1()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locate_puns(file, heterographic):\n",
    "    \"\"\"\n",
    "    generate pmi scores\n",
    "    the second word in the word pair with the highest pmi score in a given sentence is the pun word\n",
    "    if a given index contains a pun,\n",
    "    check the highest pmi score in the dictionary at that index & find the second word\n",
    "    Next, go into sentences at that index and find the index of that word (starting from 1, not 0)\n",
    "    Append this to the results list\n",
    "    in order to get the correct sentence id for a given pun, I need to find the sentence in the \n",
    "    \"\"\"\n",
    "    \n",
    "    formatted_results = []\n",
    "    stop = stopwords.words('english') + list(string.punctuation)\n",
    "    \"\"\"\n",
    "    if heterographic:\n",
    "       prefix = \"het\"\n",
    "    else:\n",
    "        prefix = \"hom\"\n",
    "    \"\"\" \n",
    "    # get pmi scores in the form of a dictionary for each sentence mapping each word_pair to a pmi score\n",
    "    ordered_pmi_scores, skipgram, sentences_dict = generate_pmi_scores(file)\n",
    "    \n",
    "    # this will be a dict mapping sentence_id to the pun_word_index (starting from 1, not 0) in that sentence\n",
    "    sentence_to_pun_location = dict()\n",
    "    sentences = list(sentences_dict.keys())\n",
    "    #print(sentences_dict)\n",
    "    for i in range(1, len(ordered_pmi_scores)):\n",
    "        if (len(ordered_pmi_scores[i]) == 0):\n",
    "            print(\"{} is empty\".format(i))\n",
    "            continue\n",
    "            \n",
    "        curr_dict = ordered_pmi_scores[i]\n",
    "        highest_pmi = list(curr_dict.items())[0][0] # get word_pair in current sentence with highest pmi\n",
    "        pun_index = 0\n",
    "        # now we need to find the sentence_id that this word_pair belongs to\n",
    "        \n",
    "        \n",
    "        for j in range(len(sentences)):\n",
    "            found_sentence = False\n",
    "            \n",
    "            if highest_pmi[0] and highest_pmi[1] in sentences[j].lower():\n",
    "                #print(\"{} is in {}\".format(highest_pmi, sentences[j]))\n",
    "                found_sentence = True\n",
    "                curr_sent = word_tokenize(sentences[j].lower())\n",
    "                \n",
    "                # iterate through each word in the sentence to find the index of the pun_word\n",
    "                for k in range(len(curr_sent)):\n",
    "                    #print(\"checking whether {} == {}\".format(highest_pmi[1], curr_sent[k]))\n",
    "                    if highest_pmi[1] == curr_sent[k]:\n",
    "                        #print(\"found pun index for sentence #: {}\".format(i))\n",
    "                        pun_index = k+1\n",
    "                        sentence_id = sentences_dict[sentences[j]]\n",
    "                        sentence_to_pun_location.update({sentence_id : pun_index})\n",
    "                        break\n",
    "            if found_sentence: \n",
    "                break # no need to check the rest of the sentences\n",
    "                        \n",
    "    \n",
    "    print(sentence_to_pun_location)\n",
    "        \n",
    "    \n",
    "    for pun in list(sentence_to_pun_location.items()):\n",
    "        formatted_results.append(str(pun[0]) + \" \" + str(pun[0]) + \"_\" + str(pun[1]))\n",
    "    \n",
    "    \n",
    "    return formatted_results     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 is empty\n",
      "77 is empty\n",
      "236 is empty\n",
      "237 is empty\n",
      "240 is empty\n",
      "303 is empty\n",
      "332 is empty\n",
      "622 is empty\n",
      "676 is empty\n",
      "864 is empty\n",
      "923 is empty\n",
      "944 is empty\n",
      "945 is empty\n",
      "1016 is empty\n",
      "1051 is empty\n",
      "1091 is empty\n",
      "1173 is empty\n",
      "1256 is empty\n",
      "{'het_1': 11, 'het_2': 11, 'het_4': 8, 'het_5': 4, 'het_7': 3, 'het_8': 8, 'het_9': 2, 'het_10': 6, 'het_11': 13, 'het_12': 4, 'het_13': 6, 'het_15': 3, 'het_16': 3, 'het_17': 3, 'het_19': 7, 'het_20': 5, 'het_22': 5, 'het_23': 8, 'het_24': 6, 'het_25': 9, 'het_26': 8, 'het_28': 7, 'het_31': 14, 'het_33': 23, 'het_34': 4, 'het_35': 7, 'het_36': 13, 'het_38': 10, 'het_39': 15, 'het_40': 10, 'het_42': 5, 'het_43': 3, 'het_44': 7, 'het_45': 5, 'het_46': 6, 'het_47': 3, 'het_49': 3, 'het_50': 6, 'het_51': 1, 'het_54': 6, 'het_55': 2, 'het_58': 4, 'het_62': 2, 'het_64': 10, 'het_66': 4, 'het_67': 4, 'het_69': 4, 'het_70': 16, 'het_71': 10, 'het_72': 2, 'het_73': 6, 'het_74': 11, 'het_75': 3, 'het_76': 8, 'het_77': 6, 'het_78': 9, 'het_80': 5, 'het_82': 5, 'het_87': 4, 'het_89': 7, 'het_90': 3, 'het_92': 10, 'het_94': 9, 'het_95': 4, 'het_96': 5, 'het_97': 4, 'het_99': 12, 'het_100': 4, 'het_101': 9, 'het_102': 7, 'het_105': 4, 'het_106': 9, 'het_107': 2, 'het_108': 12, 'het_111': 9, 'het_112': 8, 'het_113': 4, 'het_116': 6, 'het_117': 11, 'het_121': 6, 'het_123': 14, 'het_124': 12, 'het_125': 8, 'het_126': 21, 'het_129': 12, 'het_131': 12, 'het_133': 4, 'het_134': 11, 'het_136': 10, 'het_137': 9, 'het_138': 10, 'het_140': 4, 'het_141': 6, 'het_142': 10, 'het_143': 6, 'het_144': 4, 'het_145': 8, 'het_148': 4, 'het_150': 2, 'het_153': 13, 'het_154': 11, 'het_156': 11, 'het_157': 3, 'het_158': 4, 'het_162': 5, 'het_165': 8, 'het_166': 26, 'het_168': 5, 'het_172': 4, 'het_177': 7, 'het_181': 12, 'het_183': 9, 'het_184': 3, 'het_185': 4, 'het_188': 13, 'het_189': 10, 'het_191': 5, 'het_193': 4, 'het_194': 3, 'het_195': 5, 'het_197': 6, 'het_201': 8, 'het_202': 3, 'het_203': 9, 'het_204': 7, 'het_206': 6, 'het_208': 4, 'het_209': 9, 'het_213': 3, 'het_215': 7, 'het_216': 9, 'het_217': 10, 'het_218': 3, 'het_221': 14, 'het_222': 8, 'het_223': 8, 'het_227': 3, 'het_228': 9, 'het_229': 3, 'het_231': 2, 'het_234': 9, 'het_239': 3, 'het_242': 2, 'het_244': 3, 'het_246': 11, 'het_247': 21, 'het_248': 7, 'het_249': 13, 'het_252': 4, 'het_253': 4, 'het_254': 13, 'het_258': 9, 'het_259': 15, 'het_261': 4, 'het_263': 3, 'het_264': 5, 'het_267': 8, 'het_174': 3, 'het_270': 8, 'het_272': 12, 'het_276': 6, 'het_277': 17, 'het_278': 10, 'het_282': 6, 'het_283': 10, 'het_284': 2, 'het_285': 3, 'het_287': 6, 'het_288': 6, 'het_289': 4, 'het_291': 13, 'het_292': 9, 'het_293': 6, 'het_294': 8, 'het_298': 9, 'het_299': 4, 'het_301': 2, 'het_302': 3, 'het_303': 8, 'het_305': 2, 'het_306': 14, 'het_308': 3, 'het_309': 11, 'het_311': 5, 'het_312': 3, 'het_313': 7, 'het_314': 4, 'het_315': 9, 'het_316': 4, 'het_317': 6, 'het_318': 13, 'het_320': 2, 'het_322': 7, 'het_323': 6, 'het_324': 2, 'het_325': 3, 'het_326': 9, 'het_329': 4, 'het_332': 18, 'het_333': 11, 'het_334': 7, 'het_335': 6, 'het_336': 14, 'het_345': 15, 'het_346': 3, 'het_347': 5, 'het_348': 5, 'het_350': 3, 'het_351': 7, 'het_353': 4, 'het_354': 9, 'het_356': 8, 'het_357': 11, 'het_358': 8, 'het_360': 10, 'het_361': 7, 'het_362': 14, 'het_365': 18, 'het_366': 25, 'het_369': 8, 'het_370': 3, 'het_373': 6, 'het_374': 11, 'het_375': 5, 'het_377': 14, 'het_379': 5, 'het_381': 8, 'het_382': 19, 'het_384': 11, 'het_387': 7, 'het_388': 14, 'het_391': 14, 'het_393': 8, 'het_394': 13, 'het_396': 5, 'het_398': 8, 'het_399': 6, 'het_400': 16, 'het_409': 5, 'het_411': 2, 'het_415': 5, 'het_417': 10, 'het_421': 7, 'het_426': 16, 'het_427': 10, 'het_428': 5, 'het_429': 12, 'het_430': 5, 'het_431': 6, 'het_435': 6, 'het_436': 7, 'het_440': 9, 'het_442': 7, 'het_444': 7, 'het_445': 8, 'het_446': 28, 'het_449': 2, 'het_451': 6, 'het_454': 8, 'het_460': 2, 'het_461': 5, 'het_462': 3, 'het_463': 4, 'het_464': 3, 'het_467': 9, 'het_468': 4, 'het_470': 14, 'het_473': 5, 'het_478': 7, 'het_479': 7, 'het_480': 17, 'het_481': 22, 'het_482': 3, 'het_485': 11, 'het_486': 13, 'het_487': 2, 'het_489': 18, 'het_491': 5, 'het_236': 6, 'het_494': 9, 'het_496': 7, 'het_497': 2, 'het_498': 3, 'het_499': 10, 'het_501': 9, 'het_503': 12, 'het_504': 5, 'het_505': 3, 'het_506': 8, 'het_508': 11, 'het_509': 6, 'het_119': 7, 'het_513': 18, 'het_514': 4, 'het_518': 9, 'het_519': 7, 'het_522': 5, 'het_525': 15, 'het_528': 3, 'het_530': 4, 'het_534': 5, 'het_535': 11, 'het_537': 11, 'het_539': 19, 'het_540': 7, 'het_541': 12, 'het_542': 6, 'het_544': 4, 'het_545': 8, 'het_548': 4, 'het_549': 8, 'het_550': 9, 'het_551': 5, 'het_552': 11, 'het_553': 6, 'het_556': 8, 'het_559': 6, 'het_560': 8, 'het_561': 4, 'het_562': 8, 'het_564': 14, 'het_566': 7, 'het_567': 7, 'het_578': 6, 'het_579': 3, 'het_583': 9, 'het_586': 5, 'het_587': 8, 'het_588': 2, 'het_589': 9, 'het_592': 13, 'het_597': 22, 'het_600': 5, 'het_602': 4, 'het_605': 13, 'het_607': 10, 'het_608': 10, 'het_609': 14, 'het_610': 8, 'het_612': 5, 'het_618': 10, 'het_621': 17, 'het_622': 5, 'het_624': 10, 'het_625': 3, 'het_627': 13, 'het_629': 4, 'het_630': 5, 'het_632': 10, 'het_635': 12, 'het_636': 6, 'het_638': 2, 'het_639': 5, 'het_640': 5, 'het_643': 6, 'het_644': 4, 'het_645': 11, 'het_646': 3, 'het_647': 12, 'het_649': 4, 'het_652': 24, 'het_653': 2, 'het_656': 14, 'het_662': 8, 'het_668': 6, 'het_669': 8, 'het_671': 4, 'het_672': 11, 'het_674': 4, 'het_675': 6, 'het_677': 5, 'het_684': 5, 'het_685': 19, 'het_689': 5, 'het_690': 7, 'het_691': 10, 'het_692': 3, 'het_693': 3, 'het_694': 6, 'het_699': 4, 'het_1039': 4, 'het_701': 3, 'het_703': 15, 'het_709': 5, 'het_855': 8, 'het_711': 20, 'het_716': 13, 'het_718': 18, 'het_720': 5, 'het_721': 3, 'het_722': 6, 'het_723': 4, 'het_724': 3, 'het_730': 4, 'het_731': 6, 'het_732': 7, 'het_733': 5, 'het_737': 7, 'het_742': 3, 'het_743': 12, 'het_746': 13, 'het_749': 16, 'het_750': 7, 'het_753': 3, 'het_905': 12, 'het_756': 3, 'het_757': 14, 'het_759': 10, 'het_760': 10, 'het_762': 9, 'het_764': 4, 'het_765': 23, 'het_768': 5, 'het_769': 5, 'het_770': 3, 'het_771': 11, 'het_773': 3, 'het_775': 2, 'het_776': 4, 'het_782': 11, 'het_783': 21, 'het_794': 6, 'het_795': 11, 'het_798': 6, 'het_799': 15, 'het_159': 14, 'het_803': 6, 'het_804': 3, 'het_808': 6, 'het_810': 3, 'het_814': 17, 'het_818': 5, 'het_821': 15, 'het_824': 2, 'het_825': 6, 'het_828': 7, 'het_829': 10, 'het_830': 10, 'het_834': 9, 'het_836': 12, 'het_837': 7, 'het_840': 19, 'het_842': 9, 'het_844': 17, 'het_410': 3, 'het_851': 4, 'het_852': 4, 'het_859': 9, 'het_861': 4, 'het_866': 12, 'het_867': 11, 'het_871': 11, 'het_872': 4, 'het_875': 2, 'het_880': 13, 'het_149': 1, 'het_883': 19, 'het_885': 6, 'het_886': 8, 'het_888': 10, 'het_889': 4, 'het_891': 9, 'het_892': 12, 'het_893': 11, 'het_897': 4, 'het_898': 18, 'het_265': 3, 'het_901': 5, 'het_903': 4, 'het_904': 9, 'het_907': 9, 'het_908': 8, 'het_909': 4, 'het_911': 4, 'het_913': 11, 'het_914': 5, 'het_915': 4, 'het_920': 7, 'het_924': 6, 'het_928': 12, 'het_929': 5, 'het_930': 2, 'het_934': 3, 'het_935': 12, 'het_943': 9, 'het_944': 4, 'het_949': 8, 'het_950': 12, 'het_952': 10, 'het_953': 5, 'het_955': 5, 'het_958': 5, 'het_964': 3, 'het_969': 10, 'het_971': 19, 'het_418': 3, 'het_980': 5, 'het_827': 14, 'het_527': 30, 'het_990': 3, 'het_993': 3, 'het_997': 6, 'het_998': 3, 'het_1001': 17, 'het_1002': 5, 'het_1003': 7, 'het_1004': 2, 'het_1005': 9, 'het_1006': 11, 'het_1007': 14, 'het_1008': 14, 'het_1011': 3, 'het_1016': 17, 'het_1017': 6, 'het_1018': 27, 'het_1019': 5, 'het_1020': 14, 'het_1022': 13, 'het_1024': 9, 'het_1029': 3, 'het_1030': 3, 'het_1031': 8, 'het_1034': 6, 'het_1036': 22, 'het_706': 1, 'het_1038': 3, 'het_1043': 21, 'het_1045': 8, 'het_1046': 11, 'het_1049': 4, 'het_1050': 8, 'het_1051': 16, 'het_1053': 7, 'het_1054': 4, 'het_1055': 8, 'het_1057': 3, 'het_1062': 5, 'het_1063': 6, 'het_1065': 20, 'het_1066': 22, 'het_1067': 3, 'het_1070': 8, 'het_1073': 5, 'het_1074': 3, 'het_1075': 11, 'het_708': 15, 'het_1078': 9, 'het_1079': 3, 'het_1080': 15, 'het_1086': 9, 'het_1090': 7, 'het_1091': 3, 'het_1094': 13, 'het_1096': 7, 'het_1097': 11, 'het_1101': 8, 'het_1105': 10, 'het_1106': 7, 'het_524': 5, 'het_1109': 2, 'het_1112': 4, 'het_1114': 10, 'het_1116': 5, 'het_1122': 5, 'het_1126': 5, 'het_1128': 2, 'het_1129': 5, 'het_1132': 11, 'het_1133': 6, 'het_1136': 2, 'het_1137': 6, 'het_1141': 3, 'het_1142': 8, 'het_1148': 3, 'het_1152': 6, 'het_1155': 9, 'het_1159': 4, 'het_1160': 9, 'het_1171': 9, 'het_1174': 8, 'het_1175': 7, 'het_1177': 7, 'het_1180': 9, 'het_1183': 4, 'het_1185': 5, 'het_1186': 11, 'het_1190': 5, 'het_1195': 18, 'het_1197': 10, 'het_1199': 9, 'het_1200': 11, 'het_1201': 3, 'het_1202': 9, 'het_1203': 3, 'het_1204': 14, 'het_1206': 13, 'het_1209': 7, 'het_224': 4, 'het_1214': 11, 'het_1216': 3, 'het_1219': 7, 'het_1221': 7, 'het_1225': 7, 'het_1227': 8, 'het_1232': 7, 'het_1237': 7, 'het_1242': 5, 'het_457': 7, 'het_1244': 4, 'het_1248': 15, 'het_1251': 3, 'het_1254': 19, 'het_1255': 4, 'het_1259': 4, 'het_1260': 4, 'het_1262': 5, 'het_1264': 13, 'het_1265': 3, 'het_1269': 4, 'het_1272': 2, 'het_1275': 10, 'het_1278': 10, 'het_1103': 18, 'het_1282': 8, 'het_1283': 8, 'het_1287': 9, 'het_1289': 11, 'het_1290': 5, 'het_1292': 15, 'het_1300': 6, 'het_1302': 4, 'het_1304': 4, 'het_1305': 5, 'het_1308': 8, 'het_1310': 6, 'het_1311': 4, 'het_1312': 8, 'het_1314': 4, 'het_1317': 3, 'het_1319': 6, 'het_1327': 8, 'het_1330': 4, 'het_1336': 3, 'het_1345': 4, 'het_1346': 10, 'het_1349': 8, 'het_1353': 5, 'het_1354': 5, 'het_1356': 5, 'het_1362': 5, 'het_1363': 4, 'het_1365': 14, 'het_1367': 14, 'het_1369': 5, 'het_1371': 3, 'het_1378': 10, 'het_1382': 12, 'het_1386': 5, 'het_1387': 19, 'het_956': 2, 'het_1389': 12, 'het_1390': 3, 'het_1393': 6, 'het_1395': 12, 'het_1396': 4, 'het_1397': 12, 'het_1398': 7, 'het_1399': 10, 'het_1403': 7, 'het_1407': 4, 'het_1412': 3, 'het_1413': 8, 'het_1417': 12, 'het_1418': 6, 'het_1421': 18, 'het_554': 1, 'het_1423': 18, 'het_1425': 15, 'het_1427': 8, 'het_1428': 18, 'het_1431': 4, 'het_1433': 9, 'het_1436': 5, 'het_1437': 10, 'het_1438': 3, 'het_1442': 5, 'het_1443': 4, 'het_1446': 3, 'het_1454': 5, 'het_1455': 11, 'het_1457': 5, 'het_1459': 6, 'het_1464': 11, 'het_1466': 17, 'het_1467': 3, 'het_1468': 6, 'het_1470': 14, 'het_1473': 3, 'het_1476': 5, 'het_1477': 7, 'het_1478': 5, 'het_1479': 6, 'het_1480': 4, 'het_1481': 5, 'het_1486': 6, 'het_1488': 8, 'het_1489': 3, 'het_1490': 7, 'het_1491': 11, 'het_386': 5, 'het_1494': 7, 'het_1495': 17, 'het_1497': 11, 'het_1499': 5, 'het_1348': 6, 'het_1501': 7, 'het_1052': 10, 'het_1503': 3, 'het_1504': 18, 'het_1505': 6, 'het_1506': 3, 'het_1507': 6, 'het_1508': 13, 'het_1509': 9, 'het_1511': 6, 'het_1512': 11, 'het_1514': 17, 'het_1515': 14, 'het_1525': 3, 'het_1527': 14, 'het_1529': 10, 'het_1531': 12, 'het_1537': 8, 'het_1538': 3, 'het_1539': 13, 'het_1542': 6, 'het_1544': 10, 'het_1546': 35, 'het_1547': 7, 'het_1549': 11, 'het_1553': 11, 'het_1554': 2, 'het_1556': 8, 'het_1558': 4, 'het_1560': 6, 'het_1561': 4, 'het_1563': 8, 'het_1571': 8, 'het_1575': 11, 'het_1576': 6, 'het_1579': 15, 'het_1581': 4, 'het_1583': 7, 'het_1586': 7, 'het_1588': 6, 'het_1594': 4, 'het_1597': 12, 'het_1599': 10, 'het_1604': 4, 'het_310': 5, 'het_1617': 3, 'het_1622': 12, 'het_453': 5, 'het_1631': 11, 'het_1633': 2, 'het_1634': 11, 'het_1635': 13, 'het_1636': 7, 'het_1638': 10, 'het_1639': 11, 'het_1642': 12, 'het_1644': 3, 'het_1645': 4, 'het_1651': 6, 'het_1653': 5, 'het_1095': 10, 'het_1658': 12, 'het_1659': 4, 'het_1630': 12, 'het_1662': 7, 'het_1668': 15, 'het_1671': 10, 'het_1674': 6, 'het_1676': 4, 'het_1677': 4, 'het_933': 2, 'het_1683': 17, 'het_1686': 4, 'het_1697': 4, 'het_1698': 9, 'het_483': 2, 'het_1700': 7, 'het_1704': 11, 'het_1707': 6, 'het_219': 12, 'het_1715': 7, 'het_1718': 10, 'het_617': 2, 'het_1722': 5, 'het_1724': 14, 'het_1726': 3, 'het_1728': 5, 'het_1729': 2, 'het_1730': 3, 'het_1734': 8, 'het_1738': 4, 'het_1742': 7, 'het_1743': 4, 'het_1744': 9, 'het_1745': 4, 'het_1749': 9, 'het_1750': 11, 'het_1754': 13, 'het_1762': 5, 'het_1765': 9, 'het_1766': 6, 'het_1767': 9, 'het_1768': 7, 'het_1771': 14, 'het_1773': 17, 'het_1775': 3, 'het_1776': 5, 'het_1777': 5, 'het_1778': 15, 'het_1779': 7, 'het_1780': 8}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93 is empty\n",
      "115 is empty\n",
      "153 is empty\n",
      "263 is empty\n",
      "278 is empty\n",
      "377 is empty\n",
      "539 is empty\n",
      "548 is empty\n",
      "662 is empty\n",
      "689 is empty\n",
      "697 is empty\n",
      "712 is empty\n",
      "725 is empty\n",
      "820 is empty\n",
      "860 is empty\n",
      "891 is empty\n",
      "905 is empty\n",
      "976 is empty\n",
      "1014 is empty\n",
      "1025 is empty\n",
      "1052 is empty\n",
      "1077 is empty\n",
      "1079 is empty\n",
      "1112 is empty\n",
      "1131 is empty\n",
      "1204 is empty\n",
      "1207 is empty\n",
      "1227 is empty\n",
      "1335 is empty\n",
      "1388 is empty\n",
      "1460 is empty\n",
      "1541 is empty\n",
      "1551 is empty\n",
      "1562 is empty\n",
      "1563 is empty\n",
      "{'hom_1': 11, 'hom_2': 9, 'hom_3': 3, 'hom_4': 5, 'hom_5': 12, 'hom_7': 14, 'hom_8': 10, 'hom_9': 7, 'hom_10': 5, 'hom_11': 3, 'hom_14': 5, 'hom_15': 4, 'hom_18': 4, 'hom_19': 4, 'hom_20': 8, 'hom_21': 10, 'hom_22': 8, 'hom_23': 5, 'hom_25': 6, 'hom_26': 5, 'hom_27': 9, 'hom_28': 3, 'hom_29': 9, 'hom_30': 5, 'hom_31': 11, 'hom_33': 4, 'hom_34': 7, 'hom_38': 14, 'hom_39': 19, 'hom_41': 16, 'hom_43': 5, 'hom_44': 9, 'hom_45': 6, 'hom_46': 6, 'hom_47': 10, 'hom_48': 10, 'hom_51': 11, 'hom_52': 7, 'hom_53': 9, 'hom_55': 12, 'hom_58': 11, 'hom_60': 4, 'hom_61': 3, 'hom_62': 9, 'hom_63': 10, 'hom_67': 4, 'hom_69': 2, 'hom_70': 12, 'hom_71': 19, 'hom_72': 10, 'hom_73': 5, 'hom_74': 3, 'hom_75': 3, 'hom_76': 10, 'hom_78': 12, 'hom_80': 5, 'hom_82': 6, 'hom_85': 3, 'hom_86': 7, 'hom_87': 12, 'hom_88': 3, 'hom_90': 9, 'hom_91': 12, 'hom_92': 2, 'hom_93': 14, 'hom_96': 11, 'hom_97': 5, 'hom_99': 2, 'hom_100': 14, 'hom_103': 7, 'hom_104': 8, 'hom_106': 3, 'hom_107': 11, 'hom_108': 13, 'hom_109': 6, 'hom_110': 2, 'hom_112': 8, 'hom_115': 6, 'hom_116': 3, 'hom_117': 5, 'hom_119': 6, 'hom_120': 3, 'hom_124': 4, 'hom_127': 16, 'hom_130': 9, 'hom_132': 10, 'hom_133': 8, 'hom_135': 3, 'hom_137': 2, 'hom_138': 5, 'hom_139': 3, 'hom_140': 10, 'hom_141': 12, 'hom_142': 12, 'hom_144': 3, 'hom_145': 2, 'hom_148': 7, 'hom_149': 5, 'hom_150': 8, 'hom_152': 3, 'hom_154': 5, 'hom_156': 7, 'hom_159': 11, 'hom_161': 2, 'hom_163': 11, 'hom_164': 6, 'hom_167': 3, 'hom_170': 11, 'hom_172': 10, 'hom_173': 3, 'hom_174': 13, 'hom_177': 8, 'hom_178': 11, 'hom_181': 15, 'hom_183': 7, 'hom_185': 6, 'hom_186': 2, 'hom_188': 3, 'hom_189': 11, 'hom_196': 5, 'hom_197': 11, 'hom_198': 4, 'hom_199': 6, 'hom_201': 12, 'hom_202': 8, 'hom_203': 7, 'hom_204': 7, 'hom_207': 10, 'hom_209': 7, 'hom_214': 7, 'hom_217': 3, 'hom_218': 10, 'hom_221': 10, 'hom_224': 6, 'hom_225': 7, 'hom_226': 3, 'hom_50': 9, 'hom_230': 10, 'hom_231': 10, 'hom_233': 5, 'hom_234': 6, 'hom_235': 10, 'hom_236': 3, 'hom_238': 3, 'hom_240': 4, 'hom_245': 3, 'hom_247': 11, 'hom_250': 6, 'hom_254': 3, 'hom_256': 11, 'hom_258': 4, 'hom_259': 4, 'hom_260': 6, 'hom_268': 7, 'hom_269': 4, 'hom_273': 4, 'hom_274': 2, 'hom_278': 13, 'hom_279': 5, 'hom_281': 20, 'hom_282': 17, 'hom_283': 11, 'hom_284': 4, 'hom_286': 5, 'hom_287': 4, 'hom_288': 16, 'hom_289': 3, 'hom_293': 11, 'hom_294': 11, 'hom_296': 1, 'hom_300': 4, 'hom_301': 15, 'hom_304': 12, 'hom_305': 3, 'hom_310': 6, 'hom_313': 11, 'hom_315': 9, 'hom_317': 2, 'hom_318': 6, 'hom_319': 2, 'hom_327': 7, 'hom_328': 12, 'hom_329': 4, 'hom_330': 2, 'hom_334': 10, 'hom_341': 10, 'hom_342': 4, 'hom_346': 6, 'hom_353': 1, 'hom_354': 3, 'hom_356': 3, 'hom_361': 8, 'hom_363': 6, 'hom_365': 8, 'hom_367': 13, 'hom_368': 6, 'hom_369': 11, 'hom_371': 6, 'hom_373': 21, 'hom_374': 2, 'hom_375': 13, 'hom_378': 3, 'hom_380': 20, 'hom_383': 3, 'hom_386': 10, 'hom_388': 9, 'hom_389': 6, 'hom_392': 5, 'hom_393': 5, 'hom_394': 10, 'hom_308': 37, 'hom_397': 14, 'hom_400': 23, 'hom_401': 13, 'hom_403': 5, 'hom_404': 6, 'hom_405': 13, 'hom_406': 3, 'hom_408': 11, 'hom_24': 2, 'hom_411': 5, 'hom_412': 21, 'hom_414': 13, 'hom_415': 8, 'hom_416': 8, 'hom_417': 7, 'hom_418': 4, 'hom_421': 11, 'hom_423': 4, 'hom_424': 8, 'hom_429': 8, 'hom_430': 16, 'hom_431': 9, 'hom_433': 2, 'hom_437': 6, 'hom_438': 6, 'hom_440': 13, 'hom_442': 17, 'hom_443': 10, 'hom_444': 4, 'hom_447': 7, 'hom_261': 6, 'hom_452': 8, 'hom_457': 6, 'hom_458': 10, 'hom_462': 6, 'hom_464': 3, 'hom_468': 2, 'hom_470': 4, 'hom_473': 8, 'hom_479': 5, 'hom_482': 3, 'hom_483': 3, 'hom_485': 15, 'hom_486': 8, 'hom_487': 14, 'hom_488': 11, 'hom_491': 5, 'hom_493': 3, 'hom_495': 3, 'hom_496': 13, 'hom_497': 7, 'hom_498': 4, 'hom_500': 22, 'hom_503': 8, 'hom_504': 4, 'hom_506': 9, 'hom_508': 8, 'hom_511': 11, 'hom_512': 5, 'hom_513': 11, 'hom_514': 10, 'hom_520': 9, 'hom_522': 11, 'hom_526': 4, 'hom_529': 2, 'hom_535': 4, 'hom_538': 3, 'hom_542': 9, 'hom_543': 3, 'hom_358': 2, 'hom_545': 8, 'hom_549': 6, 'hom_550': 3, 'hom_553': 2, 'hom_554': 18, 'hom_555': 4, 'hom_557': 3, 'hom_558': 9, 'hom_563': 3, 'hom_565': 7, 'hom_568': 8, 'hom_569': 4, 'hom_571': 10, 'hom_574': 2, 'hom_575': 11, 'hom_81': 9, 'hom_579': 13, 'hom_582': 5, 'hom_584': 8, 'hom_587': 2, 'hom_591': 7, 'hom_593': 1, 'hom_594': 4, 'hom_595': 9, 'hom_596': 4, 'hom_395': 11, 'hom_602': 7, 'hom_604': 19, 'hom_605': 6, 'hom_609': 7, 'hom_612': 4, 'hom_614': 6, 'hom_615': 6, 'hom_620': 5, 'hom_621': 7, 'hom_622': 5, 'hom_624': 3, 'hom_630': 2, 'hom_631': 2, 'hom_635': 7, 'hom_636': 2, 'hom_637': 4, 'hom_644': 9, 'hom_454': 2, 'hom_655': 9, 'hom_656': 3, 'hom_657': 13, 'hom_659': 5, 'hom_661': 6, 'hom_662': 10, 'hom_664': 2, 'hom_666': 2, 'hom_667': 7, 'hom_670': 5, 'hom_671': 2, 'hom_672': 4, 'hom_673': 9, 'hom_674': 15, 'hom_675': 6, 'hom_677': 6, 'hom_679': 8, 'hom_681': 2, 'hom_686': 4, 'hom_687': 5, 'hom_688': 6, 'hom_693': 11, 'hom_697': 6, 'hom_698': 6, 'hom_699': 10, 'hom_704': 11, 'hom_707': 3, 'hom_711': 9, 'hom_712': 7, 'hom_714': 15, 'hom_726': 6, 'hom_732': 16, 'hom_735': 2, 'hom_738': 5, 'hom_742': 4, 'hom_237': 8, 'hom_748': 8, 'hom_749': 11, 'hom_751': 5, 'hom_758': 3, 'hom_760': 11, 'hom_762': 7, 'hom_763': 9, 'hom_764': 2, 'hom_768': 8, 'hom_769': 8, 'hom_771': 2, 'hom_775': 5, 'hom_778': 4, 'hom_779': 5, 'hom_780': 16, 'hom_785': 16, 'hom_787': 53, 'hom_788': 6, 'hom_791': 11, 'hom_792': 5, 'hom_796': 2, 'hom_799': 14, 'hom_800': 9, 'hom_804': 10, 'hom_805': 3, 'hom_807': 3, 'hom_810': 5, 'hom_812': 11, 'hom_814': 12, 'hom_815': 5, 'hom_348': 7, 'hom_820': 3, 'hom_821': 10, 'hom_833': 4, 'hom_834': 13, 'hom_835': 20, 'hom_836': 7, 'hom_841': 17, 'hom_844': 4, 'hom_845': 5, 'hom_848': 7, 'hom_852': 8, 'hom_855': 13, 'hom_857': 7, 'hom_858': 3, 'hom_860': 9, 'hom_862': 14, 'hom_863': 5, 'hom_864': 9, 'hom_866': 13, 'hom_868': 14, 'hom_871': 6, 'hom_854': 5, 'hom_876': 9, 'hom_881': 11, 'hom_882': 4, 'hom_884': 8, 'hom_885': 2, 'hom_889': 13, 'hom_890': 7, 'hom_891': 14, 'hom_901': 5, 'hom_903': 4, 'hom_906': 7, 'hom_580': 2, 'hom_910': 2, 'hom_912': 10, 'hom_913': 5, 'hom_914': 8, 'hom_915': 8, 'hom_916': 8, 'hom_920': 6, 'hom_922': 14, 'hom_924': 4, 'hom_926': 9, 'hom_929': 13, 'hom_934': 10, 'hom_937': 8, 'hom_939': 8, 'hom_941': 17, 'hom_945': 14, 'hom_946': 13, 'hom_948': 6, 'hom_16': 6, 'hom_956': 8, 'hom_958': 6, 'hom_959': 5, 'hom_960': 7, 'hom_961': 5, 'hom_964': 1, 'hom_966': 3, 'hom_968': 3, 'hom_969': 11, 'hom_710': 19, 'hom_979': 19, 'hom_981': 8, 'hom_984': 17, 'hom_985': 2, 'hom_766': 13, 'hom_991': 26, 'hom_992': 21, 'hom_993': 9, 'hom_994': 13, 'hom_996': 3, 'hom_998': 10, 'hom_999': 8, 'hom_267': 2, 'hom_1009': 7, 'hom_1010': 9, 'hom_1012': 10, 'hom_1013': 7, 'hom_1014': 11, 'hom_1015': 14, 'hom_1016': 6, 'hom_1017': 3, 'hom_1018': 12, 'hom_1020': 4, 'hom_1025': 4, 'hom_1027': 15, 'hom_1030': 5, 'hom_1031': 11, 'hom_1032': 12, 'hom_1034': 5, 'hom_1036': 11, 'hom_1039': 3, 'hom_1040': 8, 'hom_717': 9, 'hom_1050': 9, 'hom_1051': 2, 'hom_1052': 9, 'hom_1054': 2, 'hom_1055': 6, 'hom_1056': 24, 'hom_1062': 10, 'hom_1063': 4, 'hom_1068': 17, 'hom_838': 10, 'hom_1071': 17, 'hom_1077': 9, 'hom_1080': 4, 'hom_1081': 7, 'hom_1082': 6, 'hom_1084': 2, 'hom_1085': 5, 'hom_1088': 8, 'hom_434': 4, 'hom_1092': 7, 'hom_1093': 6, 'hom_1100': 13, 'hom_1101': 8, 'hom_1105': 2, 'hom_1107': 12, 'hom_1109': 2, 'hom_1111': 12, 'hom_1113': 5, 'hom_1114': 1, 'hom_1119': 7, 'hom_1120': 13, 'hom_1122': 4, 'hom_1123': 8, 'hom_1124': 10, 'hom_1126': 5, 'hom_220': 5, 'hom_1132': 12, 'hom_1133': 3, 'hom_1136': 16, 'hom_1139': 14, 'hom_1140': 11, 'hom_1141': 4, 'hom_1144': 3, 'hom_399': 2, 'hom_1148': 5, 'hom_1151': 9, 'hom_1153': 4, 'hom_1155': 10, 'hom_1158': 9, 'hom_1164': 2, 'hom_1166': 4, 'hom_1168': 8, 'hom_1172': 3, 'hom_1173': 2, 'hom_1174': 4, 'hom_1175': 5, 'hom_1177': 8, 'hom_1178': 11, 'hom_1179': 2, 'hom_1181': 19, 'hom_1183': 16, 'hom_1187': 12, 'hom_1190': 6, 'hom_1192': 4, 'hom_1197': 4, 'hom_1206': 4, 'hom_1212': 11, 'hom_819': 15, 'hom_1214': 7, 'hom_1217': 9, 'hom_1219': 4, 'hom_1224': 8, 'hom_1225': 13, 'hom_1227': 4, 'hom_1231': 16, 'hom_1236': 9, 'hom_1237': 12, 'hom_1238': 2, 'hom_1239': 4, 'hom_1241': 4, 'hom_716': 5, 'hom_1245': 4, 'hom_1248': 14, 'hom_1249': 4, 'hom_1251': 8, 'hom_1253': 10, 'hom_1260': 21, 'hom_1261': 8, 'hom_1264': 4, 'hom_1267': 8, 'hom_1271': 8, 'hom_1277': 4, 'hom_1279': 8, 'hom_1281': 8, 'hom_1282': 9, 'hom_1291': 4, 'hom_1294': 7, 'hom_1295': 8, 'hom_1296': 5, 'hom_1300': 10, 'hom_1302': 9, 'hom_1309': 13, 'hom_1314': 7, 'hom_1321': 13, 'hom_1323': 9, 'hom_1324': 8, 'hom_1325': 5, 'hom_1326': 8, 'hom_1331': 4, 'hom_1336': 9, 'hom_1338': 5, 'hom_1339': 7, 'hom_1340': 7, 'hom_1344': 10, 'hom_1350': 2, 'hom_1351': 5, 'hom_1352': 4, 'hom_1354': 19, 'hom_1357': 6, 'hom_1359': 7, 'hom_1360': 4, 'hom_1363': 10, 'hom_1364': 8, 'hom_1365': 7, 'hom_1373': 9, 'hom_1375': 4, 'hom_1378': 8, 'hom_1381': 5, 'hom_1382': 5, 'hom_1387': 12, 'hom_1389': 8, 'hom_1394': 5, 'hom_1395': 12, 'hom_1400': 19, 'hom_1327': 9, 'hom_1403': 7, 'hom_1407': 4, 'hom_1408': 4, 'hom_1410': 5, 'hom_1412': 20, 'hom_1419': 4, 'hom_1275': 20, 'hom_1424': 18, 'hom_1425': 4, 'hom_1427': 3, 'hom_169': 2, 'hom_1432': 10, 'hom_1433': 5, 'hom_1435': 11, 'hom_1437': 3, 'hom_1441': 9, 'hom_1449': 9, 'hom_1450': 9, 'hom_1453': 8, 'hom_1456': 3, 'hom_1458': 4, 'hom_1459': 11, 'hom_1460': 14, 'hom_1462': 8, 'hom_1463': 2, 'hom_1320': 19, 'hom_1469': 4, 'hom_1471': 7, 'hom_1476': 7, 'hom_1477': 7, 'hom_1128': 4, 'hom_1483': 6, 'hom_1486': 5, 'hom_1488': 11, 'hom_1491': 3, 'hom_1495': 23, 'hom_1501': 7, 'hom_1502': 4, 'hom_1504': 10, 'hom_1510': 4, 'hom_1513': 6, 'hom_1514': 6, 'hom_783': 13, 'hom_1526': 9, 'hom_1527': 11, 'hom_193': 5, 'hom_1531': 9, 'hom_1534': 17, 'hom_1535': 6, 'hom_1537': 3, 'hom_1538': 7, 'hom_1540': 12, 'hom_1542': 4, 'hom_1543': 16, 'hom_1546': 7, 'hom_1547': 4, 'hom_1548': 6, 'hom_128': 10, 'hom_1551': 11, 'hom_1554': 4, 'hom_1557': 3, 'hom_1558': 4, 'hom_1559': 3, 'hom_1561': 16, 'hom_1564': 9, 'hom_1569': 3, 'hom_1570': 11, 'hom_1574': 12, 'hom_1578': 11, 'hom_1585': 7, 'hom_1586': 11, 'hom_1587': 9, 'hom_1589': 6, 'hom_1590': 9, 'hom_1593': 2, 'hom_1608': 7, 'hom_1617': 15, 'hom_1624': 9, 'hom_1625': 15, 'hom_1626': 4, 'hom_1627': 7, 'hom_1628': 6, 'hom_1637': 8, 'hom_1642': 4, 'hom_1646': 2, 'hom_1651': 16, 'hom_1653': 6, 'hom_1655': 10, 'hom_229': 9, 'hom_1660': 9, 'hom_1661': 4, 'hom_1675': 10, 'hom_1676': 6, 'hom_1677': 8, 'hom_1682': 6, 'hom_1689': 11, 'hom_1692': 12, 'hom_1697': 25, 'hom_1698': 17, 'hom_1702': 6, 'hom_1707': 3, 'hom_1708': 3, 'hom_1713': 7, 'hom_1717': 3, 'hom_1726': 14, 'hom_1730': 5, 'hom_1732': 7, 'hom_1737': 4, 'hom_1739': 3, 'hom_1742': 6, 'hom_1744': 12, 'hom_1748': 8, 'hom_1749': 17, 'hom_1754': 9, 'hom_1755': 9, 'hom_1763': 5, 'hom_1764': 13, 'hom_1767': 13, 'hom_1768': 4, 'hom_1771': 3, 'hom_1773': 13, 'hom_1780': 5, 'hom_756': 13, 'hom_1154': 1, 'hom_1786': 10, 'hom_1792': 9, 'hom_1794': 3, 'hom_1795': 3, 'hom_1797': 4, 'hom_1799': 2, 'hom_1801': 7, 'hom_1804': 8, 'hom_1806': 8, 'hom_1808': 9, 'hom_1809': 6, 'hom_1813': 5, 'hom_1828': 5, 'hom_531': 4, 'hom_1833': 11, 'hom_1834': 6, 'hom_1835': 9, 'hom_1841': 4, 'hom_1843': 6, 'hom_1844': 4, 'hom_1846': 5, 'hom_1850': 11, 'hom_1591': 13, 'hom_1855': 3, 'hom_1861': 6, 'hom_1862': 9, 'hom_1868': 9, 'hom_1871': 14, 'hom_1873': 19, 'hom_1875': 10, 'hom_1878': 7, 'hom_1086': 7, 'hom_949': 3, 'hom_1885': 13, 'hom_1886': 5, 'hom_1887': 3, 'hom_1888': 13, 'hom_1892': 4, 'hom_1894': 8, 'hom_1905': 12, 'hom_1912': 10, 'hom_1917': 5, 'hom_1918': 17, 'hom_1922': 10, 'hom_1304': 2, 'hom_1924': 3, 'hom_1926': 4, 'hom_1928': 5, 'hom_1938': 2, 'hom_1941': 4, 'hom_1942': 6, 'hom_1943': 5, 'hom_1945': 4, 'hom_1946': 9, 'hom_1947': 7, 'hom_1954': 6, 'hom_1956': 10, 'hom_1957': 11, 'hom_266': 12, 'hom_1965': 13, 'hom_1966': 4, 'hom_976': 12, 'hom_774': 5, 'hom_1971': 3, 'hom_1976': 6, 'hom_1977': 7, 'hom_1980': 4, 'hom_1982': 12, 'hom_1985': 7, 'hom_339': 6, 'hom_1987': 9, 'hom_1988': 14, 'hom_1989': 11, 'hom_1990': 11, 'hom_1367': 15, 'hom_1994': 3, 'hom_1995': 7, 'hom_1996': 5, 'hom_2001': 7, 'hom_2004': 5, 'hom_2006': 4, 'hom_2011': 7, 'hom_2015': 10, 'hom_2016': 7, 'hom_2019': 6, 'hom_2021': 3, 'hom_2023': 13, 'hom_2027': 3, 'hom_2028': 6, 'hom_2029': 9, 'hom_2033': 23, 'hom_2034': 13, 'hom_2039': 7, 'hom_2042': 4, 'hom_1545': 19, 'hom_2051': 7, 'hom_2055': 11, 'hom_2057': 8, 'hom_2058': 11, 'hom_2060': 17, 'hom_2066': 5, 'hom_2067': 9, 'hom_1991': 5, 'hom_2071': 9, 'hom_2072': 9, 'hom_2077': 8, 'hom_2078': 8, 'hom_2079': 5, 'hom_2081': 7, 'hom_2084': 6, 'hom_2085': 14, 'hom_2087': 10, 'hom_2089': 9, 'hom_402': 13, 'hom_2092': 10, 'hom_2096': 4, 'hom_2099': 9, 'hom_2102': 5, 'hom_2104': 9, 'hom_2107': 3, 'hom_2114': 13, 'hom_2119': 2, 'hom_165': 11, 'hom_2126': 8, 'hom_2129': 9, 'hom_2141': 12, 'hom_2143': 2, 'hom_2158': 7, 'hom_2163': 5, 'hom_2166': 3, 'hom_2169': 10, 'hom_2170': 11, 'hom_917': 8, 'hom_2179': 10, 'hom_2184': 11, 'hom_432': 18, 'hom_2186': 16, 'hom_1439': 3, 'hom_2200': 6, 'hom_2207': 4, 'hom_1785': 5, 'hom_2209': 3, 'hom_2212': 13, 'hom_2213': 4, 'hom_2216': 6, 'hom_2217': 8, 'hom_2220': 8, 'hom_2231': 5, 'hom_2233': 2, 'hom_2235': 9, 'hom_2237': 11, 'hom_2238': 9, 'hom_2239': 4, 'hom_2240': 14, 'hom_2241': 13, 'hom_2250': 9}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Completes subtask 2 (pun location)\n",
    "Outputs the results for heterographic and homographic puns to two seperate files\n",
    "\n",
    "In order to run the scoring system for the heterographic puns:\n",
    "open terminal and change directory to ~/datasets/scoring/bin\n",
    "then, enter (your file paths will vary): \n",
    "java de.tudarmstadt.ukp.semeval2017.task7.scorer.PunScorer -l ~/Desktop/GitHub/NLP-Final-Project/datasets/data/test/subtask2-heterographic-test.gold ~/Desktop/GitHub/NLP-Final-Project/system_output/subtask2_heterographic.txt ~/Desktop/GitHub/NLP-Final-Project/scorer_results/subtask2_heterographic_results.txt\n",
    "\n",
    "for homographic puns:\n",
    "open terminal and change directory to ~/datasets/scoring/bin\n",
    "then, enter (your file paths will vary): \n",
    "java de.tudarmstadt.ukp.semeval2017.task7.scorer.PunScorer -l ~/Desktop/GitHub/NLP-Final-Project/datasets/data/test/subtask2-homographic-test.gold ~/Desktop/GitHub/NLP-Final-Project/system_output/subtask2_homographic.txt ~/Desktop/GitHub/NLP-Final-Project/scorer_results/subtask2_homographic_results.txt\n",
    "\"\"\"\n",
    "def subtask2():\n",
    "    locate_pun_heterographic = locate_puns('datasets/data/test/subtask2-heterographic-test.xml', True)\n",
    "    with open('system_output/subtask2_heterographic.txt', 'w') as filehandle:\n",
    "        for pun_location in locate_pun_heterographic:\n",
    "            filehandle.write('{}\\n'.format(pun_location))\n",
    "\n",
    "    locate_pun_homographic = locate_puns('datasets/data/test/subtask2-homographic-test.xml', False)\n",
    "    with open('system_output/subtask2_homographic.txt', 'w') as filehandle:\n",
    "        for pun_location in locate_pun_homographic:\n",
    "            filehandle.write('{}\\n'.format(pun_location))\n",
    "            \n",
    "subtask2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class SkipGramModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size (int): vocabulary size\n",
    "            embedding_dim (int): the dimension of word embeddings\n",
    "        \"\"\"\n",
    "        ### INSERT YOUR CODE BELOW ###\n",
    "        #self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        #self.linear = torch.nn.Linear(1, vocab_size)\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = torch.nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "        ### INSERT YOUR CODE ABOVE ###\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Perform the forward pass of the skip-gram model.\n",
    "        \n",
    "        Args:\n",
    "            inputs (torch.LongTensor): input tensor containing batches of word ids [Bx1]\n",
    "        Returns:\n",
    "            outputs (torch.FloatTensor): output tensor with unnormalized probabilities over the vocabulary [BxV]\n",
    "        \"\"\"\n",
    "        ### INSERT YOUR CODE BELOW ###\n",
    "        embeds = self.embedding(inputs)\n",
    "        #embeds = self.embedding(inputs)\n",
    "        outputs = self.linear(embeds)\n",
    "        outputs=outputs\n",
    "        #output = F.log_softmax(self.linear(embeds), dim=1)\n",
    "        ### INSERT YOUR CODE ABOVE ###\n",
    "        return outputs\n",
    "    \n",
    "    def save_embeddings(self, voc, path):\n",
    "        \"\"\"\n",
    "        Save the embedding matrix to a specified path.\n",
    "        \n",
    "        Args:\n",
    "            voc (Vocabulary): the Vocabulary object for id-to-token mapping\n",
    "            path (str): the location of the target file\n",
    "        \"\"\"\n",
    "        ### INSERT YOUR CODE BELOW ###\n",
    "        embeds = self.embedding.weight.data.cpu().numpy()\n",
    "        f = open(path, 'w')\n",
    "        f.write(str(vocab_size) + ' ' + str(embedding_dim) + '\\n')\n",
    "        \n",
    "        for idx in range(len(embeds)):\n",
    "            word = voc.idx2w[idx]\n",
    "            embedding = ' '.join(map(str,embeds[idx]))\n",
    "            f.write(word + ' '+ embedding + '\\n')\n",
    "        ### INSERT YOUR CODE ABOVE ###\n",
    "        print(\"Successfuly saved to {}\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
