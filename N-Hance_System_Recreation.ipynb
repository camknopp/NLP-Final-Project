{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Hance system recreation for SemEval2017 Task 7\n",
    "\n",
    "Names: Jorge Mazariegos & Cameron Knopp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/camknopp/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# imports statements\n",
    "import time\n",
    "import string\n",
    "import itertools\n",
    "import operator\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import nltk\n",
    "import re\n",
    "from nltk.collocations import *\n",
    "from scipy.stats import iqr\n",
    "from statistics import median\n",
    "from collections import defaultdict, OrderedDict, Counter\n",
    "from bs4 import BeautifulSoup\n",
    "#from gensim.models import KeyedVectors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "set(stopwords.words('english'))\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-paper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess should take in the datasets (.xml) and prepare them to be used\n",
    "def preprocess(data):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sentences (list(str)):\n",
    "    Returns: a list of tokens and a list of tokenized sentences\n",
    "\n",
    "    \"\"\"\n",
    "    #######################################################\n",
    "    # initialize variables to use in preprocess\n",
    "    #######################################################\n",
    "    puns = []\n",
    "    tokens = []\n",
    "    stop = stopwords.words('english') + list(string.punctuation)\n",
    "        \n",
    "    #######################################################\n",
    "    # Given a sentence, tokenize it and append it to a list\n",
    "    #######################################################\n",
    "    for sentence in data:\n",
    "        puns.append(word_tokenize(sentence.lower())) # creates the list of all sentences\n",
    "        \n",
    "    #######################################################\n",
    "    # Every sentence is tokenized, but let's grab each\n",
    "    # individual word to make a vocab out of.\n",
    "    #######################################################\n",
    "    for sentence in puns:\n",
    "        for word in sentence:\n",
    "            if(word.isalpha()): # filter out punctuation\n",
    "                tokens.append(word)\n",
    "    #######################################################\n",
    "    # Remove stop words from tokens\n",
    "    #######################################################\n",
    "    tokens_with_stop_words = tokens\n",
    "    tokens = [token for token in tokens_with_stop_words if token not in stop]\n",
    "\n",
    "    return tokens, puns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(file):\n",
    "\n",
    "    # DATA PROCESSING #\n",
    "    #######################################################\n",
    "    # Open the dataset/'s we will be using and process the\n",
    "    # text within to be used by our code.\n",
    "    #######################################################\n",
    "    #f = open('datasets/data/test/subtask1-heterographic-test.xml', 'r', encoding = 'utf8')\n",
    "    \n",
    "    f = open(file, 'r', encoding = 'utf8')\n",
    "    data = f.read()\n",
    "\n",
    "    #######################################################\n",
    "    # Using Beautiful Soup we can easily extract the puns\n",
    "    # from the given datasets.\n",
    "    #######################################################\n",
    "    soup = BeautifulSoup(data, 'xml')\n",
    "    ids = soup.find_all('text')\n",
    "    words = soup.find_all('word')\n",
    "    \n",
    "    \"\"\"\n",
    "    add each word to the dictionary pa\n",
    "    \"\"\"\n",
    "    #######################################################\n",
    "    # Create a list of all sentences within the dataset to hand\n",
    "    # over to our preprocess function\n",
    "    #######################################################\n",
    "    wurd = \"\"\n",
    "    sentence = \"\"\n",
    "    sentences = []\n",
    "    pun_list = []\n",
    "    \n",
    "    #this will be a dictionary of {sentence : sentence_id}\n",
    "    sentences_dict = dict()\n",
    "    \n",
    "    # this will contain the mapping of sentence id -> tuple(word, word_id)\n",
    "    word_dict = dict(list())\n",
    "    \n",
    "    for i in range(len(words)):\n",
    "        if words[i].parent['id'] in word_dict:\n",
    "            word_dict[words[i].parent['id']].append((words[i].string, words[i]['id']))\n",
    "        else:\n",
    "            word_dict[words[i].parent['id']] = [(words[i].string, words[i]['id'])]\n",
    "        \n",
    "    #print(word_dict)\n",
    "    \n",
    "    for i in range(len(ids)):\n",
    "        for line in ids[i]:\n",
    "            for word in line:\n",
    "                if(word != '\\n' or word == '\\''):\n",
    "                    if(word.isalpha()): # If not punctuation\n",
    "                        wurd = word\n",
    "                        if(sentence == \"\"): # If the start of the sentence\n",
    "                            sentence = sentence + wurd\n",
    "                        else: # If not the start of the sentence\n",
    "                            sentence = sentence + \" \" + wurd\n",
    "                    else: # If punctuation we don't want to put a space between the character and it.\n",
    "                        wurd = word\n",
    "                        sentence = sentence + wurd\n",
    "                    wurd = \"\" # clear the current word\n",
    "        sentences.append(sentence) # append the created string sentence to our list.\n",
    "        \n",
    "        sentences_dict.update({sentence : ids[i]['id']}) # map the sentence to it's sentence id in the .xml file\n",
    "        sentence = \"\"\n",
    "        \n",
    "    #######################################################\n",
    "    # Create a list of tokens to make a vocabulary of and\n",
    "    # create a list of sentences to create make word pairs\n",
    "    # from.\n",
    "    #######################################################\n",
    "    \n",
    "    \n",
    "    tokens, pun_list = preprocess(sentences)\n",
    "    return tokens, pun_list, sentences_dict, word_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, special_tokens=None):\n",
    "        self.w2idx = {}\n",
    "        self.idx2w = {}\n",
    "        self.w2cnt = defaultdict(int)\n",
    "        self.special_tokens = special_tokens\n",
    "        if self.special_tokens is not None:\n",
    "            self.add_tokens(special_tokens)\n",
    "\n",
    "    def add_tokens(self, tokens):\n",
    "        for token in tokens:\n",
    "            self.add_token(token)\n",
    "            self.w2cnt[token] += 1\n",
    "\n",
    "    def add_token(self, token):\n",
    "        if token not in self.w2idx:\n",
    "            cur_len = len(self)\n",
    "            self.w2idx[token] = cur_len\n",
    "            self.idx2w[cur_len] = token\n",
    "\n",
    "    def prune(self, min_cnt=2):\n",
    "        to_remove = set([token for token in self.w2idx if self.w2cnt[token] < min_cnt])\n",
    "        if self.special_tokens is not None:\n",
    "            to_remove = to_remove.difference(set(self.special_tokens))\n",
    "        \n",
    "        for token in to_remove:\n",
    "            self.w2cnt.pop(token)\n",
    "            \n",
    "        self.w2idx = {token: idx for idx, token in enumerate(self.w2cnt.keys())}\n",
    "        self.idx2w = {idx: token for token, idx in self.w2idx.items()}\n",
    "    \n",
    "    def __contains__(self, item):\n",
    "        return item in self.w2idx\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        if isinstance(item, str):\n",
    "            return self.w2idx[item]\n",
    "        elif isinstance(item , int):\n",
    "            return self.idx2w[item]\n",
    "        else:\n",
    "            raise TypeError(\"Supported indices are int and str\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return(len(self.w2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "# Using skipgrams we can create the wordpairs described\n",
    "# in the N-Hance research paper.\n",
    "#######################################################\n",
    "\n",
    "class SkipGramDataset(Dataset):\n",
    "    def __init__(self, data, vocab, skip_window=3):\n",
    "        super().__init__()\n",
    "\n",
    "        #######################################################\n",
    "        # Unlike before, data will be a list of strings handed\n",
    "        # all at once.\n",
    "        #######################################################\n",
    "        self.vocab = vocab\n",
    "        self.data = data\n",
    "        # set skip_window to the length of the longest sentence in the data set\n",
    "        self.skip_window =  max(data, key=len)\n",
    "        self.pairs = self._generate_pairs(data, skip_window)\n",
    "        \n",
    "        \n",
    "    #######################################################\n",
    "    # generate word pairs given list of lists of words representing each sentence\n",
    "    #######################################################\n",
    "    def _generate_pairs(self, data, skip_window):\n",
    "\n",
    "        pairs = [[]]  # list of word pairs for each sentence\n",
    "        curr_sentence_pairs = [] # list of word pairs for current sentence\n",
    "        pruned_pairs = []\n",
    "        \n",
    "\n",
    "        for sent in data: \n",
    "            for i in range(len(sent)):\n",
    "                for j in range(-skip_window, skip_window + 1):\n",
    "                    context_idx = i + j\n",
    "                    if j == 0 or context_idx < 0 or context_idx >= len(sent):\n",
    "                        continue\n",
    "                    if sent[i] not in self.vocab or sent[context_idx] not in self.vocab:\n",
    "                        continue\n",
    "                        \n",
    "                    # only add in this sentence if the reverse does not already exist in the list\n",
    "                    if (sent[context_idx], sent[i]) not in curr_sentence_pairs:\n",
    "                        curr_sentence_pairs.append((sent[i], sent[context_idx]))\n",
    "                    \n",
    "            pairs.append(curr_sentence_pairs.copy()) # need to append a copy so that it is not cleared with we call clear() in the next line\n",
    "            curr_sentence_pairs.clear()\n",
    "                    \n",
    "        return pairs\n",
    "    \n",
    "    #######################################################\n",
    "    # returns the list of word_pairs for the sentence at the given index\n",
    "    #######################################################\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        pair = self.pairs[idx]\n",
    "\n",
    "        #pair = [self.vocab[t] for t in pair]\n",
    "        #pair = [self.vocab.__getitem__(t) for t in pair]\n",
    "        return pair\n",
    "    \n",
    "    #######################################################\n",
    "    # returns the number of sentences\n",
    "    #######################################################\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pmi_scores(file):\n",
    "    \"\"\"\n",
    "    returns a list of dictionaries (one for each sentence) of {word_pair : pmi_score}\n",
    "    each dictionary is ordered from highest to lowest pmi score\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "        Because pun words seldom appear in Wikipedia, we added test datasets to guarantee words co-occur at least once and thus the\n",
    "        system is able to compute PMI scores for each\n",
    "        word pair. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    corpus = open(\"corpus.txt\").read().replace('\\n','')\n",
    "    corpus = word_tokenize(corpus)\n",
    "\n",
    "    # Call the bigram method within NLTK\n",
    "    bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "    # Apply the Moby Dick text to the above bigram method\n",
    "    bgram_finder = BigramCollocationFinder.from_words(corpus, 20)\n",
    "\n",
    "    # Score the bigrams\n",
    "    bgrams_scored = bgram_finder.score_ngrams(bigram_measures.raw_freq)\n",
    "    \n",
    "    # print(bgrams_scored[:5])\n",
    "\n",
    "    # Tokenize dataset and Create a Vocabulary using the tokens\n",
    "    tokens, pun_list, sentences_dict, word_dict = data_process(file)\n",
    "    voc = Vocabulary()\n",
    "    voc.add_tokens(tokens)\n",
    "    \n",
    "    # create skipgram dataset using vocab and puns\n",
    "    skipgram = SkipGramDataset(pun_list, voc, skip_window=2)\n",
    "    \n",
    "    # create a Counter object to get counts of individual words    \n",
    "    stop = stopwords.words('english') + list(string.punctuation) + [\"''\", '\"\"', \"...\"]\n",
    "    all_sentences = list(itertools.chain.from_iterable(pun_list.copy()))\n",
    "    all_sentences = [word for word in all_sentences if word not in stop]\n",
    "    \n",
    "    word_counts = Counter(all_sentences)\n",
    "    total_words = len(all_sentences)\n",
    "    \n",
    "    # get list of lists of word_pairs for each sentence\n",
    "    word_pairs = skipgram.pairs.copy()\n",
    "    word_pairs = [[(a,b) for (a,b) in sent] for sent in word_pairs] \n",
    "        \n",
    "    # create Counter object to get counts for each word_pair\n",
    "    all_word_pairs= list(itertools.chain.from_iterable(word_pairs.copy())) # join all sentences together\n",
    "    all_word_pairs = [(a,b) for (a,b) in all_word_pairs] \n",
    "    total_word_pairs = len(all_word_pairs)\n",
    "    word_pair_count = Counter(all_word_pairs)\n",
    "    \n",
    "    bgrams_pmi = [pmi_pair for pmi_pair in bgrams_scored if pmi_pair[0] in all_word_pairs]\n",
    "    \n",
    "    # create a list of dictionaries for each sentence { word_pair : pmi_score }\n",
    "    pmi_scores = list(dict())\n",
    "    \n",
    "    # now we will calculate the PMI score for each word_pair\n",
    "    # the formula for PMI score is: log[p(x,y) / (p(x)*p(y))]\n",
    "    for i in range(skipgram.__len__()):\n",
    "        current_dict = {}\n",
    "        # for each sentence, find pmi score for each individual word_pair\n",
    "        for w_pair in word_pairs[i]:\n",
    "            for pair in bgrams_pmi:\n",
    "                if pair[0] == w_pair:\n",
    "                    current_dict.update({pair[0] : pair[1]})\n",
    "#             numerator = word_pair_count[w_pair] / total_word_pairs\n",
    "#             denominator = (word_counts[w_pair[0]] / total_words) * (word_counts[w_pair[1]] / total_words)\n",
    "#             current_pmi =  numerator / denominator\n",
    "#             current_pmi = math.log(current_pmi)\n",
    "           # current_dict.update({w_pair : current_pmi}) # add bigram's pmi score to dictionary at index i (the current sentence)\n",
    "        \n",
    "        pmi_scores.append(current_dict.copy())\n",
    "        current_dict.clear()\n",
    "        \n",
    "    \n",
    "    # now we sort the dictionary entries from highest->lowest based on value (PMI score)\n",
    "    ordered_pmi_scores = list(OrderedDict())\n",
    "    \n",
    "    for i in range(len(pmi_scores)):\n",
    "        current_dict = pmi_scores[i]\n",
    "        # convert to dictionary ordered by value (which is the pmi score in this case)\n",
    "        current_ordered_dict = OrderedDict(sorted(current_dict.items(), key=lambda x: x[1], reverse=True))\n",
    "        ordered_pmi_scores.append(current_ordered_dict.copy())\n",
    "        current_ordered_dict.clear()\n",
    "    print(\"found pmi scores\")\n",
    "   \n",
    "    #print(ordered_pmi_scores)\n",
    "    return ordered_pmi_scores, skipgram, word_dict, sentences_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_puns(file, heterographic):\n",
    "    \"\"\"\n",
    "    create word_pairs for sentences in given file\n",
    "    calculate pmi scores for all given word_pairs\n",
    "    calculate the interquartile range for the pmi scores of word_pairs in each sentence\n",
    "    find the median value of the interquartile ranges across all sentences in the given dataset\n",
    "    for each sentence, if the highest pmi score - second highest pmi score > median interquartile range ...\n",
    "    (cont.) then that means that that sentence contains a pun\n",
    "    \"\"\"\n",
    "    \n",
    "    # homographic pun 5 would be referred to as hom5 in the final list (this is based on the N-Hance system's guidelines)\n",
    "    if heterographic:\n",
    "        prefix = \"het\"\n",
    "    else:\n",
    "        prefix = \"hom\"\n",
    "    \n",
    "    # get pmi scores for all word_pairs in the file\n",
    "    ordered_pmi_scores, skipgram = generate_pmi_scores(file)[:2]\n",
    "    \n",
    "    # now we need to find the interquartile range for each dictionary in the list using iqr from scipy.stats\n",
    "    iqr_values = []\n",
    "    \n",
    "    for dictionary in ordered_pmi_scores:\n",
    "        iqr_values.append(iqr(list(dictionary.values())))\n",
    "    \n",
    "    \n",
    "    # now we take the median of these iqr values and take that as our iqr value of the current dataset\n",
    "    median_iqr = median(iqr_values)\n",
    "    \n",
    "    # this will contain a 0 or 1 at each sentence id, \n",
    "    # 1 = contains pun; 0 = does not contain pun\n",
    "    contains_pun = []\n",
    "     \n",
    "    for i in range(1, len(ordered_pmi_scores)):\n",
    "        curr_dict = list(ordered_pmi_scores[i].items())\n",
    "        \n",
    "        if len(curr_dict) > 1 :\n",
    "            # if the difference between the highest pmi score and second highest pmi score (cont.)\n",
    "            #... is greater than the median iqr, then the sentence contains a pun\n",
    "            if float(curr_dict[0][1] - curr_dict[1][1]) > median_iqr:\n",
    "                contains_pun.append(prefix + \"_\" + str(i) + \" 1\" )\n",
    "            else:\n",
    "                contains_pun.append(prefix + \"_\" + str(i) + \" 0\" )\n",
    "        else:\n",
    "            contains_pun.append(prefix + \"_\" + str(i) + \" 0\" )        \n",
    "\n",
    "    # returning more than just one value because these other values are needed in subtask 2 later on\n",
    "    return contains_pun, ordered_pmi_scores, skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found pmi scores\n",
      "found pmi scores\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Completes subtask 1 (pun detection)\n",
    "Outputs the results for heterographic and homographic puns to two seperate files\n",
    "\n",
    "In order to run the scoring system for the heterographic puns:\n",
    "open terminal and change directory to ~/datasets/scoring/bin\n",
    "then, type (your file paths will vary):\n",
    "java de.tudarmstadt.ukp.semeval2017.task7.scorer.PunScorer -d ~/Desktop/GitHub/NLP-Final-Project/datasets/data/test/subtask1-heterographic-test.gold ~/Desktop/GitHub/NLP-Final-Project/system_output/subtask1_heterographic.txt ~/Desktop/GitHub/NLP-Final-Project/scorer_results/subtask1_heterographic_results.txt\n",
    "\n",
    "for homographic puns:\n",
    "open terminal and change directory to ~/datasets/scoring/bin\n",
    "then, type (your file paths will vary): \n",
    "java de.tudarmstadt.ukp.semeval2017.task7.scorer.PunScorer -d ~/Desktop/GitHub/NLP-Final-Project/datasets/data/test/subtask1-homographic-test.gold ~/Desktop/GitHub/NLP-Final-Project/system_output/subtask1_homographic.txt ~/Desktop/GitHub/NLP-Final-Project/scorer_results/subtask1_homographic_results.txt\n",
    "\n",
    "\"\"\"\n",
    "def subtask1():\n",
    "    contains_pun_heterographic = detect_puns('datasets/data/test/subtask1-heterographic-test.xml', True)[0]\n",
    "    with open('system_output/subtask1_heterographic.txt', 'w') as filehandle:\n",
    "        for pun_result in contains_pun_heterographic:\n",
    "            filehandle.write('{}\\n'.format(pun_result))\n",
    "\n",
    "    contains_pun_homographic = detect_puns('datasets/data/test/subtask1-homographic-test.xml', False)[0]\n",
    "    with open('system_output/subtask1_homographic.txt', 'w') as filehandle:\n",
    "        for pun_result in contains_pun_homographic:\n",
    "            filehandle.write('{}\\n'.format(pun_result))\n",
    "            \n",
    "subtask1()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locate_puns(file, heterographic):\n",
    "    \"\"\"\n",
    "    generate pmi scores\n",
    "    the second word in the word pair with the highest pmi score in a given sentence is the pun word\n",
    "    if a given index contains a pun,\n",
    "    check the highest pmi score in the dictionary at that index & find the second word\n",
    "    Append this to the results list\n",
    "    in order to get the correct sentence id for a given pun, I need to find the sentence in the \n",
    "    \"\"\"\n",
    "    \n",
    "    formatted_results = []\n",
    "    stop = stopwords.words('english') + list(string.punctuation)\n",
    "   \n",
    "    # get pmi scores in the form of a dictionary for each sentence mapping each word_pair to a pmi score\n",
    "    ordered_pmi_scores, skipgram, word_dict, sentences_dict = generate_pmi_scores(file)\n",
    "    \n",
    "    # this will be a dict mapping sentence_id to the pun_word_index (starting from 1, not 0) in that sentence\n",
    "    sent2punidx = dict()\n",
    "    sentences = list(sentences_dict.keys())\n",
    "\n",
    "    for i in range(1, len(ordered_pmi_scores)):\n",
    "        if (len(ordered_pmi_scores[i]) == 0):\n",
    "            continue\n",
    "            \n",
    "        curr_dict = ordered_pmi_scores[i]\n",
    "        highest_pmi = list(curr_dict.items())[0][0] # get word_pair in current sentence with highest pmi\n",
    "        pun_index = 0\n",
    "        # now we need to find the sentence_id that this word_pair belongs to\n",
    "        \n",
    "        for j in range(len(sentences)):\n",
    "            found_sentence = False\n",
    "            \n",
    "            if highest_pmi[0] and highest_pmi[1] in sentences[j].lower():\n",
    "                found_sentence = True\n",
    "                curr_sent = word_tokenize(sentences[j].lower())\n",
    "                sentence_id = sentences_dict[sentences[j]]\n",
    "              \n",
    "                for word in word_dict[sentence_id]:\n",
    "                    if word[0] == highest_pmi[1]:\n",
    "                        pun_index = word[1]\n",
    "                        sent2punidx[sentence_id] = pun_index\n",
    "                        break\n",
    "                    \n",
    "            # no need to check the rest of the sentences since we already found where the pun is located\n",
    "            if found_sentence: \n",
    "                break \n",
    "                        \n",
    "\n",
    "   # print(list(sent2punidx.items()))\n",
    "        \n",
    "    for pun in list(sent2punidx.items()):\n",
    "        formatted_results.append(str(pun[0]) + \" \" + str(pun[1]))\n",
    "#     for pun in list(sentence_to_pun_location.items()):\n",
    "#         formatted_results.append(str(pun[0]) + \" \" + str(pun[0]) + \"_\" + str(pun[1]))\n",
    "    \n",
    "    #print(formatted_results)\n",
    "    \n",
    "    return formatted_results     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Completes subtask 2 (pun location)\n",
    "Outputs the results for heterographic and homographic puns to two seperate files\n",
    "\n",
    "In order to run the scoring system for the heterographic puns:\n",
    "open terminal and change directory to ~/datasets/scoring/bin\n",
    "then, enter (your file paths will vary): \n",
    "java de.tudarmstadt.ukp.semeval2017.task7.scorer.PunScorer -l ~/Desktop/GitHub/NLP-Final-Project/datasets/data/test/subtask2-heterographic-test.gold ~/Desktop/GitHub/NLP-Final-Project/system_output/subtask2_heterographic.txt ~/Desktop/GitHub/NLP-Final-Project/scorer_results/subtask2_heterographic_results.txt\n",
    "\n",
    "for homographic puns:\n",
    "open terminal and change directory to ~/datasets/scoring/bin\n",
    "then, enter (your file paths will vary): \n",
    "java de.tudarmstadt.ukp.semeval2017.task7.scorer.PunScorer -l ~/Desktop/GitHub/NLP-Final-Project/datasets/data/test/subtask2-homographic-test.gold ~/Desktop/GitHub/NLP-Final-Project/system_output/subtask2_homographic.txt ~/Desktop/GitHub/NLP-Final-Project/scorer_results/subtask2_homographic_results.txt\n",
    "\"\"\"\n",
    "def subtask2():\n",
    "    locate_pun_heterographic = locate_puns('datasets/data/test/subtask2-heterographic-test.xml', True)\n",
    "    with open('system_output/subtask2_heterographic.txt', 'w') as filehandle:\n",
    "        for pun_location in locate_pun_heterographic:\n",
    "            filehandle.write('{}\\n'.format(pun_location))\n",
    "\n",
    "    locate_pun_homographic = locate_puns('datasets/data/test/subtask2-homographic-test.xml', False)\n",
    "    with open('system_output/subtask2_homographic.txt', 'w') as filehandle:\n",
    "        for pun_location in locate_pun_homographic:\n",
    "            filehandle.write('{}\\n'.format(pun_location))\n",
    "            \n",
    "subtask2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class SkipGramModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size (int): vocabulary size\n",
    "            embedding_dim (int): the dimension of word embeddings\n",
    "        \"\"\"\n",
    "        ### INSERT YOUR CODE BELOW ###\n",
    "        #self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        #self.linear = torch.nn.Linear(1, vocab_size)\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = torch.nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "        ### INSERT YOUR CODE ABOVE ###\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Perform the forward pass of the skip-gram model.\n",
    "        \n",
    "        Args:\n",
    "            inputs (torch.LongTensor): input tensor containing batches of word ids [Bx1]\n",
    "        Returns:\n",
    "            outputs (torch.FloatTensor): output tensor with unnormalized probabilities over the vocabulary [BxV]\n",
    "        \"\"\"\n",
    "        ### INSERT YOUR CODE BELOW ###\n",
    "        embeds = self.embedding(inputs)\n",
    "        #embeds = self.embedding(inputs)\n",
    "        outputs = self.linear(embeds)\n",
    "        outputs=outputs\n",
    "        #output = F.log_softmax(self.linear(embeds), dim=1)\n",
    "        ### INSERT YOUR CODE ABOVE ###\n",
    "        return outputs\n",
    "    \n",
    "    def save_embeddings(self, voc, path):\n",
    "        \"\"\"\n",
    "        Save the embedding matrix to a specified path.\n",
    "        \n",
    "        Args:\n",
    "            voc (Vocabulary): the Vocabulary object for id-to-token mapping\n",
    "            path (str): the location of the target file\n",
    "        \"\"\"\n",
    "        ### INSERT YOUR CODE BELOW ###\n",
    "        embeds = self.embedding.weight.data.cpu().numpy()\n",
    "        f = open(path, 'w')\n",
    "        f.write(str(vocab_size) + ' ' + str(embedding_dim) + '\\n')\n",
    "        \n",
    "        for idx in range(len(embeds)):\n",
    "            word = voc.idx2w[idx]\n",
    "            embedding = ' '.join(map(str,embeds[idx]))\n",
    "            f.write(word + ' '+ embedding + '\\n')\n",
    "        ### INSERT YOUR CODE ABOVE ###\n",
    "        print(\"Successfuly saved to {}\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_puns(file):\n",
    "    \"\"\" \n",
    "    process the .xml file\n",
    "    Find the pun word for each sentence (the one when word.sense == '2')\n",
    "    get the first relevant sense of the word by using pywsd simple lesk method\n",
    "        *** this will the overlap between the pun word sentence and the dictionary entries\n",
    "    for the second sense of the pun word, extract all senses and synyonms for given pun word\n",
    "    to measure the similarity of two words, use word2vec and take cosine of vectors of those words\n",
    "    word2vec model created using gensim library and fed using wikipedia data. Vec size of 128-dimensional and window size of 10\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
