{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Hance System Recreation for SemEval2017 Task 7\n",
    "\n",
    "Names: Jorge Mazariegos & Cameron Knopp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/camknopp/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/camknopp/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/camknopp/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# imports statements\n",
    "import time\n",
    "import string\n",
    "import itertools\n",
    "import operator\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from pymagnitude import *\n",
    "from nltk.collocations import *\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "from scipy.stats import iqr\n",
    "from statistics import median\n",
    "from collections import defaultdict, OrderedDict, Counter\n",
    "from bs4 import BeautifulSoup\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "set(stopwords.words('english'))\n",
    "  \n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-paper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pywsd.lesk import simple_lesk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess should take in the datasets (.xml) and prepare them to be used\n",
    "def preprocess(data):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sentences (list(str)):\n",
    "    Returns: a list of tokens and a list of tokenized sentences\n",
    "\n",
    "    \"\"\"\n",
    "    #######################################################\n",
    "    # initialize variables to use in preprocess\n",
    "    #######################################################\n",
    "    puns = []\n",
    "    tokens = []\n",
    "    stop = stopwords.words('english') + list(string.punctuation)\n",
    "        \n",
    "    #######################################################\n",
    "    # Given a sentence, tokenize it and append it to a list\n",
    "    #######################################################\n",
    "    for sentence in data:\n",
    "        puns.append(word_tokenize(sentence.lower())) # creates the list of all sentences\n",
    "        \n",
    "    #######################################################\n",
    "    # Every sentence is tokenized, but let's grab each\n",
    "    # individual word to make a vocab out of.\n",
    "    #######################################################\n",
    "    for sentence in puns:\n",
    "        for word in sentence:\n",
    "            if(word.isalpha()): # filter out punctuation\n",
    "                tokens.append(word)\n",
    "    #######################################################\n",
    "    # Remove stop words from tokens\n",
    "    #######################################################\n",
    "    tokens_with_stop_words = tokens\n",
    "    tokens = [token for token in tokens_with_stop_words if token not in stop]\n",
    "\n",
    "    return tokens, puns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(file):\n",
    "    \"\"\"\n",
    "    Processes the given xml file\n",
    "    \n",
    "    input:\n",
    "        - file: an xml file containing the pun sentences\n",
    "    output:\n",
    "        - tokens: tokenized sentences w/ stopwords removed\n",
    "        - pun_list: lists of words in each sentence w/ stopwords\n",
    "        - word_dict: a mapping of sentence id -> list(tuple(word, word_id)) \n",
    "        - sentences_dict: a mapping of {str(sentence) : sentence_id}\n",
    "    \"\"\"\n",
    "\n",
    "    # DATA PROCESSING #\n",
    "    #######################################################\n",
    "    # Open the dataset/'s we will be using and process the\n",
    "    # text within to be used by our code.\n",
    "    #######################################################\n",
    "    #f = open('datasets/data/test/subtask1-heterographic-test.xml', 'r', encoding = 'utf8')\n",
    "    \n",
    "    f = open(file, 'r', encoding = 'utf8')\n",
    "    data = f.read()\n",
    "\n",
    "    #######################################################\n",
    "    # Using Beautiful Soup we can easily extract the puns\n",
    "    # from the given datasets.\n",
    "    #######################################################\n",
    "    soup = BeautifulSoup(data, 'xml')\n",
    "    ids = soup.find_all('text')\n",
    "    words = soup.find_all('word')\n",
    "    \n",
    "\n",
    "    #######################################################\n",
    "    # Create a list of all sentences within the dataset to hand\n",
    "    # over to our preprocess function\n",
    "    #######################################################\n",
    "    wurd = \"\"\n",
    "    sentence = \"\"\n",
    "    sentences = []\n",
    "    pun_list = []\n",
    "    \n",
    "    #this will be a dictionary of {sentence : sentence_id}\n",
    "    sentences_dict = dict()\n",
    "    \n",
    "    # this will contain the mapping of sentence id -> list(tuple(word, word_id))\n",
    "    word_dict = dict(list())\n",
    "    \n",
    "    for i in range(len(words)):\n",
    "        if words[i].parent['id'] in word_dict:\n",
    "            word_dict[words[i].parent['id']].append((words[i], words[i]['id']))\n",
    "        else:\n",
    "            word_dict[words[i].parent['id']] = [(words[i], words[i]['id'])]\n",
    "            \n",
    "    for i in range(len(ids)):\n",
    "        for line in ids[i]:\n",
    "            for word in line:\n",
    "                if(word != '\\n' or word == '\\''):\n",
    "                    if(word.isalpha()): # If not punctuation\n",
    "                        wurd = word\n",
    "                        if(sentence == \"\"): # If the start of the sentence\n",
    "                            sentence = sentence + wurd\n",
    "                        else: # If not the start of the sentence\n",
    "                            sentence = sentence + \" \" + wurd\n",
    "                    else: # If punctuation we don't want to put a space between the character and it.\n",
    "                        wurd = word\n",
    "                        sentence = sentence + wurd\n",
    "                    wurd = \"\" # clear the current word\n",
    "        sentences.append(sentence) # append the created string sentence to our list.\n",
    "        \n",
    "        sentences_dict.update({sentence : ids[i]['id']}) # map the sentence to it's sentence id in the .xml file\n",
    "        sentence = \"\"\n",
    "        \n",
    "    #######################################################\n",
    "    # Create a list of tokens to make a vocabulary of and\n",
    "    # create a list of sentences to create make word pairs\n",
    "    # from.\n",
    "    #######################################################\n",
    "    \n",
    "    tokens, pun_list = preprocess(sentences)\n",
    "    return tokens, pun_list, sentences_dict, word_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, special_tokens=None):\n",
    "        self.w2idx = {}\n",
    "        self.idx2w = {}\n",
    "        self.w2cnt = defaultdict(int)\n",
    "        self.special_tokens = special_tokens\n",
    "        if self.special_tokens is not None:\n",
    "            self.add_tokens(special_tokens)\n",
    "\n",
    "    def add_tokens(self, tokens):\n",
    "        for token in tokens:\n",
    "            self.add_token(token)\n",
    "            self.w2cnt[token] += 1\n",
    "\n",
    "    def add_token(self, token):\n",
    "        if token not in self.w2idx:\n",
    "            cur_len = len(self)\n",
    "            self.w2idx[token] = cur_len\n",
    "            self.idx2w[cur_len] = token\n",
    "\n",
    "    def prune(self, min_cnt=2):\n",
    "        to_remove = set([token for token in self.w2idx if self.w2cnt[token] < min_cnt])\n",
    "        if self.special_tokens is not None:\n",
    "            to_remove = to_remove.difference(set(self.special_tokens))\n",
    "        \n",
    "        for token in to_remove:\n",
    "            self.w2cnt.pop(token)\n",
    "            \n",
    "        self.w2idx = {token: idx for idx, token in enumerate(self.w2cnt.keys())}\n",
    "        self.idx2w = {idx: token for token, idx in self.w2idx.items()}\n",
    "    \n",
    "    def __contains__(self, item):\n",
    "        return item in self.w2idx\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        if isinstance(item, str):\n",
    "            return self.w2idx[item]\n",
    "        elif isinstance(item , int):\n",
    "            return self.idx2w[item]\n",
    "        else:\n",
    "            raise TypeError(\"Supported indices are int and str\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return(len(self.w2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Using skipgrams we create the wordpairs as described in the N-Hance research paper.\n",
    "    The skip window is set to the length of the longest sentence in the datasets, ensuring that there is a word pair between every two words in a given sentence\n",
    "    Input:\n",
    "        - data: list of sentences\n",
    "        - vocab: vocabulary for the given sentences\n",
    "        - skip_window: unused value\n",
    "    Output\n",
    "        - list of lists of word_pairs w/ one list per sentence\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class SkipGramDataset(Dataset):\n",
    "    def __init__(self, data, vocab, skip_window=3):\n",
    "        super().__init__()\n",
    "        self.vocab = vocab\n",
    "        self.data = data\n",
    "        self.skip_window =  max(data, key=len)\n",
    "        self.pairs = self._generate_pairs(data, skip_window)\n",
    "        \n",
    "\n",
    "    def _generate_pairs(self, data, skip_window):\n",
    "        \"\"\"\n",
    "         generate word pairs given list of lists of words representing each sentence\n",
    "        \"\"\"\n",
    "        pairs = [[]]  # list of word pairs for each sentence\n",
    "        curr_sentence_pairs = [] # list of word pairs for current sentence\n",
    "\n",
    "        for sent in data: \n",
    "            for i in range(len(sent)):\n",
    "                for j in range(-skip_window, skip_window + 1):\n",
    "                    context_idx = i + j\n",
    "                    if j == 0 or context_idx < 0 or context_idx >= len(sent):\n",
    "                        continue\n",
    "                    if sent[i] not in self.vocab or sent[context_idx] not in self.vocab:\n",
    "                        continue\n",
    "                        \n",
    "                    # only add in this sentence if the reverse does not already exist in the list\n",
    "                    if (sent[context_idx], sent[i]) not in curr_sentence_pairs:\n",
    "                        curr_sentence_pairs.append((sent[i], sent[context_idx]))\n",
    "                    \n",
    "            pairs.append(curr_sentence_pairs.copy()) # need to append a copy so that it is not cleared with we call clear() in the next line\n",
    "            curr_sentence_pairs.clear()\n",
    "                    \n",
    "        return pairs\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\" \n",
    "        returns the list of word_pairs for the sentence at the given index\n",
    "        \"\"\"\n",
    "        return self.pairs[idx]\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        returns the number of sentences\n",
    "        \"\"\"\n",
    "        return len(self.pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram_prob(ngrams, word_pair, all_words):    \n",
    "    first_word = word_pair[0]\n",
    "    second_word = word_pair[1]\n",
    "    count_second_given_first = 0\n",
    "    count_first = 0\n",
    "\n",
    "    for a, b in ngrams:\n",
    "        if (a, b) == (first_word, second_word):\n",
    "            count_second_given_first+=1\n",
    "    for word in all_words:\n",
    "        if word == first_word:\n",
    "            count_first+=1\n",
    "\n",
    "    return count_second_given_first / count_first\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pmi_scores(file, use_collocations):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        - xml file which contains the sentences we will be generating PMI scores for\n",
    "        - use_collocations: boolean which says whether or not to use nltk collocations when calculating pmi scores\n",
    "            - the results for certain datasets and subtasks improve with this, while others' results are hurt\n",
    "    \n",
    "    output:\n",
    "        - ordered_pmi_scores: a list of ordered dictionaries (one for each sentence) of {word_pair : pmi_score}\n",
    "        - skipgram: a skipgram dataset of word_pairs generated from the given xml file\n",
    "        - word_dict: a mapping of sentence id -> list(tuple(word, word_id)) \n",
    "        - sentences_dict: a mapping of {str(sentence) : sentence_id}\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize dataset and Create a Vocabulary using the tokens\n",
    "    tokens, pun_list, sentences_dict, word_dict = data_process(file)\n",
    "    voc = Vocabulary()\n",
    "    voc.add_tokens(tokens)\n",
    "    \n",
    "    # create skipgram dataset using vocab and puns\n",
    "    skipgram = SkipGramDataset(pun_list, voc, skip_window=2)\n",
    "    \n",
    "    # create a Counter object to get counts of individual words    \n",
    "    stop = stopwords.words('english') + list(string.punctuation) + [\"''\", '\"\"', \"...\"]\n",
    "    all_words = list(itertools.chain.from_iterable(pun_list.copy()))\n",
    "    all_sentences = [word for word in all_words if word not in stop]\n",
    "    \n",
    "    if use_collocations:\n",
    "        bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "        finder = nltk.BigramCollocationFinder.from_words(all_sentences, 20)\n",
    "        scored = finder.score_ngrams(bigram_measures.pmi)\n",
    "        scored_list = list()\n",
    "        for val in scored:\n",
    "            scored_list.append(val)        \n",
    "   \n",
    "    word_counts = Counter(all_sentences)\n",
    "    total_words = len(all_sentences)\n",
    "    \n",
    "    # get list of lists of word_pairs for each sentence\n",
    "    word_pairs = skipgram.pairs.copy()\n",
    "    word_pairs = [[(a,b) for (a,b) in sent] for sent in word_pairs] \n",
    "        \n",
    "    # create Counter object to get counts for each word_pair\n",
    "    all_word_pairs= list(itertools.chain.from_iterable(word_pairs.copy())) # join all sentences together\n",
    "    all_word_pairs = [(a,b) for (a,b) in all_word_pairs] \n",
    "    total_word_pairs = len(all_word_pairs)\n",
    "    word_pair_count = Counter(all_word_pairs)\n",
    "    \n",
    "    # create a list of dictionaries for each sentence { word_pair : pmi_score }\n",
    "    pmi_scores = list(dict())\n",
    "    \n",
    "    # now we calculate the PMI score for each word_pair\n",
    "    # the formula for PMI score is: log[p(x,y) / (p(x)*p(y))]\n",
    "    for i in range(skipgram.__len__()):\n",
    "        current_dict = {}\n",
    "        # for each sentence, find pmi score for each individual word_pair\n",
    "        for w_pair in word_pairs[i]:\n",
    "            if use_collocations:\n",
    "                for pair in scored:\n",
    "                    if pair[0] == w_pair:\n",
    "                       # print(\"{} is equal to {}, pmi is {}\".format(pair[0], w_pair, pair[1]))\n",
    "                        current_pmi = pair[1]\n",
    "                        break\n",
    "            else:\n",
    "                numerator = word_pair_count[w_pair] / total_word_pairs\n",
    "                denominator = (word_counts[w_pair[0]] / total_words) * (word_counts[w_pair[1]] / total_words)\n",
    "                current_pmi =  numerator / denominator\n",
    "                current_pmi = math.log2(current_pmi)\n",
    "                \n",
    "            current_dict.update({w_pair : current_pmi}) # add word pair's pmi score to dictionary at index i (the current sentence)\n",
    "        \n",
    "        pmi_scores.append(current_dict.copy())\n",
    "        current_dict.clear()\n",
    "        \n",
    "    \n",
    "    # now we sort the dictionary entries from highest->lowest based on value (PMI score)\n",
    "    ordered_pmi_scores = list(OrderedDict())\n",
    "    for i in range(len(pmi_scores)):\n",
    "        current_dict = pmi_scores[i]\n",
    "        current_ordered_dict = OrderedDict(sorted(current_dict.items(), key=lambda x: x[1], reverse=True))\n",
    "        ordered_pmi_scores.append(current_ordered_dict.copy())\n",
    "        current_ordered_dict.clear()\n",
    "   \n",
    "    return ordered_pmi_scores, skipgram, word_dict, sentences_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_puns(file, heterographic):\n",
    "    \"\"\"\n",
    "    \n",
    "    detects which sentences in the given file contains puns\n",
    "    \n",
    "    input:\n",
    "        - file: xml file containing the sentences to be designated as either containing a pun or not containing a pun\n",
    "        - heterographic: boolean value asserting whether the given puns are heterographic (True) or homographic (False)\n",
    "        \n",
    "    output:\n",
    "        - contains_pun: list(str) with each string representing one line in the system output file for this subtask\n",
    "        - ordered_pmi_scores: a list of ordered dictionaries (one for each sentence) of {word_pair : pmi_score}\n",
    "        - skipgram: a skipgram dataset of word_pairs generated from the given xml file\n",
    "        \n",
    "    how this function works:\n",
    "        - create word_pairs for sentences in given file\n",
    "        - calculate pmi scores for all given word_pairs\n",
    "        - calculate the interquartile range for the pmi scores of word_pairs in each sentence\n",
    "        - find the median value of the interquartile ranges across all sentences in the given dataset\n",
    "        - for each sentence, if the highest pmi score - second highest pmi score > median interquartile range then that sentence contains a pun\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"Beginning pun detection...\")\n",
    "\n",
    "    # homographic pun 5 would be referred to as hom5 in the final list (this is based on the N-Hance system's guidelines)\n",
    "    if heterographic:\n",
    "        prefix = \"het\"\n",
    "        use_collocations = False\n",
    "    else:\n",
    "        prefix = \"hom\"\n",
    "        use_collocations = True\n",
    "    \n",
    "    # get pmi scores for all word_pairs in the file\n",
    "    ordered_pmi_scores, skipgram = generate_pmi_scores(file, use_collocations)[:2]\n",
    "    \n",
    "    # now we need to find the interquartile range for each dictionary in the list using iqr from scipy.stats\n",
    "    iqr_values = []\n",
    "    \n",
    "    for dictionary in ordered_pmi_scores:\n",
    "        iqr_values.append(iqr(list(dictionary.values())))\n",
    "    \n",
    "    \n",
    "    # now we take the median of these iqr values and take that as our iqr value of the current dataset\n",
    "    median_iqr = median(iqr_values)\n",
    "    \n",
    "    # this will contain a 0 or 1 at each sentence id, \n",
    "    # 1 = contains pun, and 0 = does not contain pun\n",
    "    contains_pun = []\n",
    "     \n",
    "    for i in range(1, len(ordered_pmi_scores)):\n",
    "        curr_dict = list(ordered_pmi_scores[i].items())\n",
    "        \n",
    "        if len(curr_dict) > 1 :\n",
    "            # if the difference between the highest pmi score and second highest pmi score (cont.)\n",
    "            #... is greater than the median iqr, then the sentence contains a pun\n",
    "            if float(curr_dict[0][1] - curr_dict[1][1]) > median_iqr:\n",
    "                contains_pun.append(prefix + \"_\" + str(i) + \" 1\" )\n",
    "            else:\n",
    "                contains_pun.append(prefix + \"_\" + str(i) + \" 0\" )\n",
    "        else:\n",
    "            contains_pun.append(prefix + \"_\" + str(i) + \" 0\" )        \n",
    "\n",
    "    print(\"Finished pun detection\")\n",
    "    return contains_pun, ordered_pmi_scores, skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning pun detection...\n",
      "Finished pun detection\n",
      "Beginning pun detection...\n",
      "Finished pun detection\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Completes subtask 1 (pun detection)\n",
    "Outputs the results for heterographic and homographic puns to two seperate files\n",
    "\n",
    "In order to run the scoring system for the heterographic puns:\n",
    "open terminal and change directory to ~/datasets/scoring/bin\n",
    "then, type (your file paths will vary):\n",
    "java de.tudarmstadt.ukp.semeval2017.task7.scorer.PunScorer -d ~/Desktop/GitHub/NLP-Final-Project/datasets/data/test/subtask1-heterographic-test.gold ~/Desktop/GitHub/NLP-Final-Project/system_output/subtask1_heterographic.txt ~/Desktop/GitHub/NLP-Final-Project/scorer_results/subtask1_heterographic_results.txt\n",
    "\n",
    "for homographic puns:\n",
    "open terminal and change directory to ~/datasets/scoring/bin\n",
    "then, type (your file paths will vary): \n",
    "java de.tudarmstadt.ukp.semeval2017.task7.scorer.PunScorer -d ~/Desktop/GitHub/NLP-Final-Project/datasets/data/test/subtask1-homographic-test.gold ~/Desktop/GitHub/NLP-Final-Project/system_output/subtask1_homographic.txt ~/Desktop/GitHub/NLP-Final-Project/scorer_results/subtask1_homographic_results.txt\n",
    "\n",
    "\"\"\"\n",
    "def subtask1():\n",
    "    contains_pun_heterographic = detect_puns('datasets/data/test/subtask1-heterographic-test.xml', True)[0]\n",
    "    with open('system_output/subtask1_heterographic.txt', 'w') as filehandle:\n",
    "        for pun_result in contains_pun_heterographic:\n",
    "            filehandle.write('{}\\n'.format(pun_result))\n",
    "\n",
    "    contains_pun_homographic = detect_puns('datasets/data/test/subtask1-homographic-test.xml', False)[0]\n",
    "    with open('system_output/subtask1_homographic.txt', 'w') as filehandle:\n",
    "        for pun_result in contains_pun_homographic:\n",
    "            filehandle.write('{}\\n'.format(pun_result))\n",
    "            \n",
    "subtask1()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locate_puns(file, heterographic):\n",
    "    \"\"\"\n",
    "    \n",
    "    locates the pun within each sentence in the file\n",
    "    \n",
    "    input:\n",
    "        - file: xml file containing the sentences in which we will locate puns\n",
    "        - heterographic: boolean value asserting whether the given puns are heterographic (True) or homographic (False)\n",
    "        \n",
    "    output:\n",
    "        - formatted_results: list(str) with each str representing being one line in the final system output file\n",
    "        \n",
    "    how this function works:\n",
    "        - generate pmi scores for the given sentence\n",
    "        - the second word in the word pair with the highest pmi score in a given sentence is the pun word\n",
    "    \n",
    "    \"\"\"\n",
    "    print(\"Beginning pun location...\")\n",
    "    \n",
    "    formatted_results = []\n",
    "    stop = stopwords.words('english') + list(string.punctuation)\n",
    "   \n",
    "    # get pmi scores in the form of a dictionary for each sentence mapping each word_pair to a pmi score\n",
    "    ordered_pmi_scores, skipgram, word_dict, sentences_dict = generate_pmi_scores(file, heterographic)\n",
    "    \n",
    "    # this will be a dict mapping sentence_id to the pun_word_index (starting from 1, not 0) in that sentence\n",
    "    sent2punidx = dict()\n",
    "    sentences = list(sentences_dict.keys())\n",
    "\n",
    "    for i in range(1, len(ordered_pmi_scores)):\n",
    "        if (len(ordered_pmi_scores[i]) == 0):\n",
    "            continue\n",
    "            \n",
    "        curr_dict = ordered_pmi_scores[i]\n",
    "        highest_pmi = list(curr_dict.items())[0][0] # get word_pair in current sentence with highest pmi\n",
    "        pun_index = 0\n",
    "        \n",
    "        # find the sentence_id that this word_pair belongs to\n",
    "        for j in range(len(sentences)):\n",
    "            found_sentence = False\n",
    "            \n",
    "            if highest_pmi[0] and highest_pmi[1] in sentences[j].lower():\n",
    "                found_sentence = True\n",
    "                curr_sent = word_tokenize(sentences[j].lower())\n",
    "                sentence_id = sentences_dict[sentences[j]]\n",
    "              \n",
    "                for word in word_dict[sentence_id]:\n",
    "                    if word[0].string == highest_pmi[1]:\n",
    "                        pun_index = word[1]\n",
    "                        sent2punidx[sentence_id] = pun_index\n",
    "                        break\n",
    "                    \n",
    "            # no need to check the rest of the sentences since we already found where the pun is located\n",
    "            if found_sentence: \n",
    "                break \n",
    "                                \n",
    "    for pun in list(sent2punidx.items()):\n",
    "        formatted_results.append(str(pun[0]) + \" \" + str(pun[1]))\n",
    "    \n",
    "    print(\"Finished locating puns\")\n",
    "\n",
    "    return formatted_results     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning pun location...\n",
      "Finished locating puns\n",
      "Beginning pun location...\n",
      "Finished locating puns\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Completes subtask 2 (pun location)\n",
    "Outputs the results for heterographic and homographic puns to two seperate files\n",
    "\n",
    "In order to run the scoring system for the heterographic puns:\n",
    "open terminal and change directory to ~/datasets/scoring/bin\n",
    "then, enter (your file paths will vary): \n",
    "java de.tudarmstadt.ukp.semeval2017.task7.scorer.PunScorer -l ~/Desktop/GitHub/NLP-Final-Project/datasets/data/test/subtask2-heterographic-test.gold ~/Desktop/GitHub/NLP-Final-Project/system_output/subtask2_heterographic.txt ~/Desktop/GitHub/NLP-Final-Project/scorer_results/subtask2_heterographic_results.txt\n",
    "\n",
    "for homographic puns:\n",
    "open terminal and change directory to ~/datasets/scoring/bin\n",
    "then, enter (your file paths will vary): \n",
    "java de.tudarmstadt.ukp.semeval2017.task7.scorer.PunScorer -l ~/Desktop/GitHub/NLP-Final-Project/datasets/data/test/subtask2-homographic-test.gold ~/Desktop/GitHub/NLP-Final-Project/system_output/subtask2_homographic.txt ~/Desktop/GitHub/NLP-Final-Project/scorer_results/subtask2_homographic_results.txt\n",
    "\"\"\"\n",
    "def subtask2():\n",
    "    locate_pun_heterographic = locate_puns('datasets/data/test/subtask2-heterographic-test.xml', True)\n",
    "    with open('system_output/subtask2_heterographic.txt', 'w') as filehandle:\n",
    "        for pun_location in locate_pun_heterographic:\n",
    "            filehandle.write('{}\\n'.format(pun_location))\n",
    "\n",
    "    locate_pun_homographic = locate_puns('datasets/data/test/subtask2-homographic-test.xml', False)\n",
    "    with open('system_output/subtask2_homographic.txt', 'w') as filehandle:\n",
    "        for pun_location in locate_pun_homographic:\n",
    "            filehandle.write('{}\\n'.format(pun_location))\n",
    "            \n",
    "subtask2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to run the code below, you will need to download the dataset at the link below:\n",
    "# http://magnitude.plasticity.ai/word2vec/medium/GoogleNews-vectors-negative300.magnitude\n",
    "# be sure to place the above dataset in the same directory as this .ipynb file\n",
    "word_vectors = Magnitude('wiki-news-300d-1M.magnitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_puns(file):\n",
    "    \"\"\" \n",
    "    \n",
    "    finds the two intended senses of the pun word in each of the given sentences\n",
    "    \n",
    "    input:\n",
    "        - file: xml file containing the sentences with the puns to be interpreted\n",
    "        \n",
    "    output:\n",
    "        - formatted_results: list(str) with each str representing being one line in the final system output file\n",
    "        \n",
    "    how this function works:\n",
    "        - first sense of the pun is found using pywsd algorithm\n",
    "        - to find the second sense of the pun, gather all the wordnet synonyms of the pun\n",
    "        - next, get the other word in the highest pmi score word pair which the pun belongs to. We'll call this the target word.\n",
    "        - use the pymagnitude word2vec to find the cosine similarity between each of the synonyms and the target word.\n",
    "        - the synonym with the highest cosine similarity to the target word is the second sense of the pun word.\n",
    "        \n",
    "    \"\"\"\n",
    "    print(\"Beginning pun interpretation...\")\n",
    "    ordered_pmi_scores, skipgram, word_dict, sentences_dict = generate_pmi_scores(file, True)\n",
    "    \n",
    "    # create a mapping of sentence (e.g., \"het_1\") to pun location and word (e.g., (\"het_1_14\", \"allegedly\") )\n",
    "    puns = dict()\n",
    "    for sent in list(word_dict.items()):\n",
    "        for word2id in sent[1]:\n",
    "            if word2id[0]['senses'] == '2':\n",
    "                puns[sent[0]] = (word2id[0]['id'], word2id[0].string)\n",
    "                \n",
    "    first_senses = dict()\n",
    "    \n",
    "    # find first sense of word using pywsd\n",
    "    for pun in list(puns.items()):\n",
    "        curr_sent = \"\"\n",
    "        curr_key = \"\"\n",
    "        for sent in list(sentences_dict.items()):\n",
    "            # find the full sentence that the given pun belongs to\n",
    "            if sent[1] == pun[0]:\n",
    "                curr_key = sent[1]\n",
    "                curr_sent = sent[0]\n",
    "                break\n",
    "        \n",
    "        curr_pun = pun[1][1]\n",
    "        sense = simple_lesk(curr_sent, curr_pun)\n",
    "        if sense is not None:\n",
    "            sense_key = sense.lemmas()[0].key()\n",
    "            first_senses[pun[1][0]] = sense_key\n",
    "            \n",
    "    punloc2wdpair = dict()\n",
    "    pun_list = list(puns.items())\n",
    "    second_senses = dict()\n",
    "\n",
    "    # get the pmi_word pair associated with the pun word in each sentence\n",
    "    for i in range(1, len(ordered_pmi_scores)):\n",
    "        curr_sent_pmi = list(ordered_pmi_scores[i].items())\n",
    "        if len(curr_sent_pmi) != 0:\n",
    "            punloc2wdpair[pun_list[i-1][1][0]] = curr_sent_pmi[0]\n",
    "        \n",
    "        \n",
    "    # find the pun word synonym that has highest cosine similarity w/ the other word in the pmi pair\n",
    "    # this synonym will be deemed the second sense for the pun word\n",
    "    for pun in list(puns.items()):\n",
    "        if pun[1][0] in list(punloc2wdpair.keys()):\n",
    "            curr_pmi_pair = punloc2wdpair[pun[1][0]]\n",
    "            non_pun_word = curr_pmi_pair[0][0]\n",
    "            pun_word = curr_pmi_pair[0][1]\n",
    "            syns = []\n",
    "            \n",
    "            for synset in wordnet.synsets(pun_word): # get all synonyms of pun word\n",
    "                for lemma in synset.lemmas():\n",
    "                    syns.append(lemma)\n",
    "\n",
    "            if len(syns) == 0:\n",
    "                # for default answer, just put in the first synset version of the pun_word\n",
    "                #second_senses[pun[1][0]] = wordnet.synsets(pun_word)[0]\n",
    "                pass\n",
    "            else:\n",
    "                highest_similar = 0\n",
    "                most_similar = None\n",
    "                for synonym in syns:\n",
    "                    curr_similarity = word_vectors.similarity(non_pun_word, synonym.name())\n",
    "                    if curr_similarity > highest_similar:\n",
    "                        highest_similarity = curr_similarity\n",
    "                        most_similar = synonym\n",
    "#                print(\"most similar word to non_pun_word {} w/ pun_word {} is {}\".format(non_pun_word, pun_word, most_similar.key()))\n",
    "                second_senses[pun[1][0]] = most_similar.key()\n",
    "        \n",
    "        \n",
    "    # put the results in a list to return, which will then be outputted to a file\n",
    "    formatted_results = []\n",
    "    for pun in list(puns.items()):\n",
    "        pun_loc = pun[1][0]\n",
    "        # if the pun word's senses were successfully found, then add them to the final results\n",
    "        if pun_loc in list(first_senses.keys()) and pun_loc in list(second_senses.keys()):\n",
    "            curr_line = str(pun_loc) + ' ' + str(first_senses[pun_loc]) + ' ' + str(second_senses[pun_loc])\n",
    "            formatted_results.append(curr_line)\n",
    "\n",
    "    print(\"Finished pun interpretation\")                               \n",
    "    return formatted_results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning pun interpretation...\n",
      "Finished pun interpretation\n",
      "Beginning pun interpretation...\n",
      "Finished pun interpretation\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Completes subtask 3 (pun interpretation)\n",
    "Outputs the results for heterographic and homographic puns to two seperate files\n",
    "\n",
    "In order to run the scoring system for the heterographic puns:\n",
    "open terminal and change directory to ~/datasets/scoring/bin\n",
    "then, enter (your file paths will vary): \n",
    "java de.tudarmstadt.ukp.semeval2017.task7.scorer.PunScorer -i ~/Desktop/GitHub/NLP-Final-Project/datasets/data/test/subtask3-heterographic-test.gold ~/Desktop/GitHub/NLP-Final-Project/system_output/subtask3_heterographic.txt ~/Desktop/GitHub/NLP-Final-Project/scorer_results/subtask3_heterographic_results.txt\n",
    "\n",
    "for homographic puns:\n",
    "open terminal and change directory to ~/datasets/scoring/bin\n",
    "then, enter (your file paths will vary): \n",
    "java de.tudarmstadt.ukp.semeval2017.task7.scorer.PunScorer -i ~/Desktop/GitHub/NLP-Final-Project/datasets/data/test/subtask3-homographic-test.gold ~/Desktop/GitHub/NLP-Final-Project/system_output/subtask3_homographic.txt ~/Desktop/GitHub/NLP-Final-Project/scorer_results/subtask3_homographic_results.txt\n",
    "\"\"\"\n",
    "\n",
    "def subtask3():\n",
    "    interpret_puns_het = interpret_puns('datasets/data/test/subtask3-heterographic-test.xml')\n",
    "    with open('system_output/subtask3_heterographic.txt', 'w') as filehandle:\n",
    "        for pun_interpretation in interpret_puns_het:\n",
    "            filehandle.write('{}\\n'.format(pun_interpretation))\n",
    "    \n",
    "    interpret_puns_hom = interpret_puns('datasets/data/test/subtask3-homographic-test.xml')\n",
    "    with open('system_output/subtask3_homographic.txt', 'w') as filehandle:\n",
    "        for pun_interpretation in interpret_puns_hom:\n",
    "            filehandle.write('{}\\n'.format(pun_interpretation))\n",
    "    \n",
    "subtask3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "new_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
