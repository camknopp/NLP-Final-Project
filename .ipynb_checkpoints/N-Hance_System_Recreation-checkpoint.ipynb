{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Hance System Recreation for SemEval2017 Task 7\n",
    "\n",
    "Names: Jorge Mazariegos & Cameron Knopp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "dlopen(/Users/camknopp/opt/anaconda3/envs/venv/lib/python3.6/site-packages/torch/_C.cpython-36m-darwin.so, 9): Symbol not found: _mkl_blas_caxpy\n  Referenced from: /Users/camknopp/opt/anaconda3/envs/venv/lib/python3.6/site-packages/torch/lib/../../../../libmkl_intel_lp64.dylib\n  Expected in: flat namespace\n in /Users/camknopp/opt/anaconda3/envs/venv/lib/python3.6/site-packages/torch/lib/../../../../libmkl_intel_lp64.dylib",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-de14b3470182>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpymagnitude\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollocations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/venv/lib/python3.6/site-packages/pymagnitude/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/third_party/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/third_party_mock/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpymagnitude\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthird_party\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melmo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mElmoEmbedder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;31m# Import SQLite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/venv/lib/python3.6/site-packages/pymagnitude/third_party/allennlp/commands/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigure\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfigure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melmo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mElmo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEvaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/venv/lib/python3.6/site-packages/pymagnitude/third_party/allennlp/commands/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigure\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfigure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melmo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mElmo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEvaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/venv/lib/python3.6/site-packages/pymagnitude/third_party/allennlp/commands/configure.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubcommand\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSubcommand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfiguration\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSubcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/venv/lib/python3.6/site-packages/pymagnitude/third_party/allennlp/common/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mParams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregistrable\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRegistrable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtee_logger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTeeLogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/venv/lib/python3.6/site-packages/pymagnitude/third_party/allennlp/common/params.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mexpr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchecks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConfigurationError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcached_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/venv/lib/python3.6/site-packages/pymagnitude/third_party/allennlp/common/checks.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=invalid-name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/venv/lib/python3.6/site-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0m_dl_flags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m __all__ += [name for name in dir(_C)\n",
      "\u001b[0;31mImportError\u001b[0m: dlopen(/Users/camknopp/opt/anaconda3/envs/venv/lib/python3.6/site-packages/torch/_C.cpython-36m-darwin.so, 9): Symbol not found: _mkl_blas_caxpy\n  Referenced from: /Users/camknopp/opt/anaconda3/envs/venv/lib/python3.6/site-packages/torch/lib/../../../../libmkl_intel_lp64.dylib\n  Expected in: flat namespace\n in /Users/camknopp/opt/anaconda3/envs/venv/lib/python3.6/site-packages/torch/lib/../../../../libmkl_intel_lp64.dylib"
     ]
    }
   ],
   "source": [
    "# imports statements\n",
    "import time\n",
    "import string\n",
    "import itertools\n",
    "import operator\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from pymagnitude import *\n",
    "from nltk.collocations import *\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import KeyedVectors\n",
    "from scipy.stats import iqr\n",
    "from statistics import median\n",
    "from collections import defaultdict, OrderedDict, Counter\n",
    "from bs4 import BeautifulSoup\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "set(stopwords.words('english'))\n",
    "  \n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-paper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pywsd.lesk import simple_lesk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess should take in the datasets (.xml) and prepare them to be used\n",
    "def preprocess(data):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        sentences (list(str)):\n",
    "    Returns: a list of tokens and a list of tokenized sentences\n",
    "\n",
    "    \"\"\"\n",
    "    #######################################################\n",
    "    # initialize variables to use in preprocess\n",
    "    #######################################################\n",
    "    puns = []\n",
    "    tokens = []\n",
    "    stop = stopwords.words('english') + list(string.punctuation)\n",
    "        \n",
    "    #######################################################\n",
    "    # Given a sentence, tokenize it and append it to a list\n",
    "    #######################################################\n",
    "    for sentence in data:\n",
    "        puns.append(word_tokenize(sentence.lower())) # creates the list of all sentences\n",
    "        \n",
    "    #######################################################\n",
    "    # Every sentence is tokenized, but let's grab each\n",
    "    # individual word to make a vocab out of.\n",
    "    #######################################################\n",
    "    for sentence in puns:\n",
    "        for word in sentence:\n",
    "            if(word.isalpha()): # filter out punctuation\n",
    "                tokens.append(word)\n",
    "    #######################################################\n",
    "    # Remove stop words from tokens\n",
    "    #######################################################\n",
    "    tokens_with_stop_words = tokens\n",
    "    tokens = [token for token in tokens_with_stop_words if token not in stop]\n",
    "\n",
    "    return tokens, puns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(file):\n",
    "\n",
    "    # DATA PROCESSING #\n",
    "    #######################################################\n",
    "    # Open the dataset/'s we will be using and process the\n",
    "    # text within to be used by our code.\n",
    "    #######################################################\n",
    "    #f = open('datasets/data/test/subtask1-heterographic-test.xml', 'r', encoding = 'utf8')\n",
    "    \n",
    "    f = open(file, 'r', encoding = 'utf8')\n",
    "    data = f.read()\n",
    "\n",
    "    #######################################################\n",
    "    # Using Beautiful Soup we can easily extract the puns\n",
    "    # from the given datasets.\n",
    "    #######################################################\n",
    "    soup = BeautifulSoup(data, 'xml')\n",
    "    ids = soup.find_all('text')\n",
    "    words = soup.find_all('word')\n",
    "    \n",
    "\n",
    "    #######################################################\n",
    "    # Create a list of all sentences within the dataset to hand\n",
    "    # over to our preprocess function\n",
    "    #######################################################\n",
    "    wurd = \"\"\n",
    "    sentence = \"\"\n",
    "    sentences = []\n",
    "    pun_list = []\n",
    "    \n",
    "    #this will be a dictionary of {sentence : sentence_id}\n",
    "    sentences_dict = dict()\n",
    "    \n",
    "    # this will contain the mapping of sentence id -> list(tuple(word, word_id))\n",
    "    word_dict = dict(list())\n",
    "    \n",
    "    for i in range(len(words)):\n",
    "        if words[i].parent['id'] in word_dict:\n",
    "            word_dict[words[i].parent['id']].append((words[i], words[i]['id']))\n",
    "        else:\n",
    "            word_dict[words[i].parent['id']] = [(words[i], words[i]['id'])]\n",
    "            \n",
    "    for i in range(len(ids)):\n",
    "        for line in ids[i]:\n",
    "            for word in line:\n",
    "                if(word != '\\n' or word == '\\''):\n",
    "                    if(word.isalpha()): # If not punctuation\n",
    "                        wurd = word\n",
    "                        if(sentence == \"\"): # If the start of the sentence\n",
    "                            sentence = sentence + wurd\n",
    "                        else: # If not the start of the sentence\n",
    "                            sentence = sentence + \" \" + wurd\n",
    "                    else: # If punctuation we don't want to put a space between the character and it.\n",
    "                        wurd = word\n",
    "                        sentence = sentence + wurd\n",
    "                    wurd = \"\" # clear the current word\n",
    "        sentences.append(sentence) # append the created string sentence to our list.\n",
    "        \n",
    "        sentences_dict.update({sentence : ids[i]['id']}) # map the sentence to it's sentence id in the .xml file\n",
    "        sentence = \"\"\n",
    "        \n",
    "    #######################################################\n",
    "    # Create a list of tokens to make a vocabulary of and\n",
    "    # create a list of sentences to create make word pairs\n",
    "    # from.\n",
    "    #######################################################\n",
    "    \n",
    "    \n",
    "    tokens, pun_list = preprocess(sentences)\n",
    "    return tokens, pun_list, sentences_dict, word_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, special_tokens=None):\n",
    "        self.w2idx = {}\n",
    "        self.idx2w = {}\n",
    "        self.w2cnt = defaultdict(int)\n",
    "        self.special_tokens = special_tokens\n",
    "        if self.special_tokens is not None:\n",
    "            self.add_tokens(special_tokens)\n",
    "\n",
    "    def add_tokens(self, tokens):\n",
    "        for token in tokens:\n",
    "            self.add_token(token)\n",
    "            self.w2cnt[token] += 1\n",
    "\n",
    "    def add_token(self, token):\n",
    "        if token not in self.w2idx:\n",
    "            cur_len = len(self)\n",
    "            self.w2idx[token] = cur_len\n",
    "            self.idx2w[cur_len] = token\n",
    "\n",
    "    def prune(self, min_cnt=2):\n",
    "        to_remove = set([token for token in self.w2idx if self.w2cnt[token] < min_cnt])\n",
    "        if self.special_tokens is not None:\n",
    "            to_remove = to_remove.difference(set(self.special_tokens))\n",
    "        \n",
    "        for token in to_remove:\n",
    "            self.w2cnt.pop(token)\n",
    "            \n",
    "        self.w2idx = {token: idx for idx, token in enumerate(self.w2cnt.keys())}\n",
    "        self.idx2w = {idx: token for token, idx in self.w2idx.items()}\n",
    "    \n",
    "    def __contains__(self, item):\n",
    "        return item in self.w2idx\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        if isinstance(item, str):\n",
    "            return self.w2idx[item]\n",
    "        elif isinstance(item , int):\n",
    "            return self.idx2w[item]\n",
    "        else:\n",
    "            raise TypeError(\"Supported indices are int and str\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return(len(self.w2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "# Using skipgrams we can create the wordpairs described\n",
    "# in the N-Hance research paper.\n",
    "#######################################################\n",
    "\n",
    "class SkipGramDataset(Dataset):\n",
    "    def __init__(self, data, vocab, skip_window=3):\n",
    "        super().__init__()\n",
    "\n",
    "        #######################################################\n",
    "        # Unlike before, data will be a list of strings handed\n",
    "        # all at once.\n",
    "        #######################################################\n",
    "        self.vocab = vocab\n",
    "        self.data = data\n",
    "        # set skip_window to the length of the longest sentence in the data set\n",
    "        self.skip_window =  max(data, key=len)\n",
    "        self.pairs = self._generate_pairs(data, skip_window)\n",
    "        \n",
    "        \n",
    "    #######################################################\n",
    "    # generate word pairs given list of lists of words representing each sentence\n",
    "    #######################################################\n",
    "    def _generate_pairs(self, data, skip_window):\n",
    "\n",
    "        pairs = [[]]  # list of word pairs for each sentence\n",
    "        curr_sentence_pairs = [] # list of word pairs for current sentence\n",
    "        pruned_pairs = []\n",
    "        \n",
    "\n",
    "        for sent in data: \n",
    "            for i in range(len(sent)):\n",
    "                for j in range(-skip_window, skip_window + 1):\n",
    "                    context_idx = i + j\n",
    "                    if j == 0 or context_idx < 0 or context_idx >= len(sent):\n",
    "                        continue\n",
    "                    if sent[i] not in self.vocab or sent[context_idx] not in self.vocab:\n",
    "                        continue\n",
    "                        \n",
    "                    # only add in this sentence if the reverse does not already exist in the list\n",
    "                    if (sent[context_idx], sent[i]) not in curr_sentence_pairs:\n",
    "                        curr_sentence_pairs.append((sent[i], sent[context_idx]))\n",
    "                    \n",
    "            pairs.append(curr_sentence_pairs.copy()) # need to append a copy so that it is not cleared with we call clear() in the next line\n",
    "            curr_sentence_pairs.clear()\n",
    "                    \n",
    "        return pairs\n",
    "    \n",
    "    #######################################################\n",
    "    # returns the list of word_pairs for the sentence at the given index\n",
    "    #######################################################\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        pair = self.pairs[idx]\n",
    "\n",
    "        #pair = [self.vocab[t] for t in pair]\n",
    "        #pair = [self.vocab.__getitem__(t) for t in pair]\n",
    "        return pair\n",
    "    \n",
    "    #######################################################\n",
    "    # returns the number of sentences\n",
    "    #######################################################\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pmi_scores(file):\n",
    "    \"\"\"\n",
    "    returns a list of dictionaries (one for each sentence) of {word_pair : pmi_score}\n",
    "    each dictionary is ordered from highest to lowest pmi score\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tokenize dataset and Create a Vocabulary using the tokens\n",
    "    tokens, pun_list, sentences_dict, word_dict = data_process(file)\n",
    "    voc = Vocabulary()\n",
    "    voc.add_tokens(tokens)\n",
    "    \n",
    "    # create skipgram dataset using vocab and puns\n",
    "    skipgram = SkipGramDataset(pun_list, voc, skip_window=2)\n",
    "    \n",
    "    # create a Counter object to get counts of individual words    \n",
    "    stop = stopwords.words('english') + list(string.punctuation) + [\"''\", '\"\"', \"...\"]\n",
    "    all_sentences = list(itertools.chain.from_iterable(pun_list.copy()))\n",
    "    all_sentences = [word for word in all_sentences if word not in stop]\n",
    "    \n",
    "    word_counts = Counter(all_sentences)\n",
    "    total_words = len(all_sentences)\n",
    "    \n",
    "    # get list of lists of word_pairs for each sentence\n",
    "    word_pairs = skipgram.pairs.copy()\n",
    "    word_pairs = [[(a,b) for (a,b) in sent] for sent in word_pairs] \n",
    "        \n",
    "    # create Counter object to get counts for each word_pair\n",
    "    all_word_pairs= list(itertools.chain.from_iterable(word_pairs.copy())) # join all sentences together\n",
    "    all_word_pairs = [(a,b) for (a,b) in all_word_pairs] \n",
    "    total_word_pairs = len(all_word_pairs)\n",
    "    word_pair_count = Counter(all_word_pairs)\n",
    "    \n",
    "    # create a list of dictionaries for each sentence { word_pair : pmi_score }\n",
    "    pmi_scores = list(dict())\n",
    "    \n",
    "    # now we calculate the PMI score for each word_pair\n",
    "    # the formula for PMI score is: log[p(x,y) / (p(x)*p(y))]\n",
    "    for i in range(skipgram.__len__()):\n",
    "        current_dict = {}\n",
    "        # for each sentence, find pmi score for each individual word_pair\n",
    "        for w_pair in word_pairs[i]:\n",
    "    \n",
    "            numerator = word_pair_count[w_pair] / total_word_pairs\n",
    "            denominator = (word_counts[w_pair[0]] / total_words) * (word_counts[w_pair[1]] / total_words)\n",
    "            current_pmi =  numerator / denominator\n",
    "            current_pmi = math.log2(current_pmi)\n",
    "            current_dict.update({w_pair : current_pmi}) # add bigram's pmi score to dictionary at index i (the current sentence)\n",
    "        \n",
    "        pmi_scores.append(current_dict.copy())\n",
    "        current_dict.clear()\n",
    "        \n",
    "    \n",
    "    # now we sort the dictionary entries from highest->lowest based on value (PMI score)\n",
    "    ordered_pmi_scores = list(OrderedDict())\n",
    "    \n",
    "    for i in range(len(pmi_scores)):\n",
    "        current_dict = pmi_scores[i]\n",
    "        # convert to dictionary ordered by value (which is the pmi score in this case)\n",
    "        current_ordered_dict = OrderedDict(sorted(current_dict.items(), key=lambda x: x[1], reverse=True))\n",
    "        ordered_pmi_scores.append(current_ordered_dict.copy())\n",
    "        current_ordered_dict.clear()\n",
    "   \n",
    "    return ordered_pmi_scores, skipgram, word_dict, sentences_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_puns(file, heterographic):\n",
    "    \"\"\"\n",
    "    create word_pairs for sentences in given file\n",
    "    calculate pmi scores for all given word_pairs\n",
    "    calculate the interquartile range for the pmi scores of word_pairs in each sentence\n",
    "    find the median value of the interquartile ranges across all sentences in the given dataset\n",
    "    for each sentence, if the highest pmi score - second highest pmi score > median interquartile range ...\n",
    "    (cont.) then that means that that sentence contains a pun\n",
    "    \"\"\"\n",
    "    print(\"Beginning pun detection...\")\n",
    "\n",
    "    # homographic pun 5 would be referred to as hom5 in the final list (this is based on the N-Hance system's guidelines)\n",
    "    if heterographic:\n",
    "        prefix = \"het\"\n",
    "    else:\n",
    "        prefix = \"hom\"\n",
    "    \n",
    "    # get pmi scores for all word_pairs in the file\n",
    "    ordered_pmi_scores, skipgram = generate_pmi_scores(file)[:2]\n",
    "    \n",
    "    # now we need to find the interquartile range for each dictionary in the list using iqr from scipy.stats\n",
    "    iqr_values = []\n",
    "    \n",
    "    for dictionary in ordered_pmi_scores:\n",
    "        iqr_values.append(iqr(list(dictionary.values())))\n",
    "    \n",
    "    \n",
    "    # now we take the median of these iqr values and take that as our iqr value of the current dataset\n",
    "    median_iqr = median(iqr_values)\n",
    "    \n",
    "    # this will contain a 0 or 1 at each sentence id, \n",
    "    # 1 = contains pun, and 0 = does not contain pun\n",
    "    contains_pun = []\n",
    "     \n",
    "    for i in range(1, len(ordered_pmi_scores)):\n",
    "        curr_dict = list(ordered_pmi_scores[i].items())\n",
    "        \n",
    "        if len(curr_dict) > 1 :\n",
    "            # if the difference between the highest pmi score and second highest pmi score (cont.)\n",
    "            #... is greater than the median iqr, then the sentence contains a pun\n",
    "            if float(curr_dict[0][1] - curr_dict[1][1]) > median_iqr:\n",
    "                contains_pun.append(prefix + \"_\" + str(i) + \" 1\" )\n",
    "            else:\n",
    "                contains_pun.append(prefix + \"_\" + str(i) + \" 0\" )\n",
    "        else:\n",
    "            contains_pun.append(prefix + \"_\" + str(i) + \" 0\" )        \n",
    "\n",
    "    print(\"Finished pun detection\")\n",
    "    return contains_pun, ordered_pmi_scores, skipgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning pun detection...\n",
      "Finished pun detection\n",
      "Beginning pun detection...\n",
      "Finished pun detection\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Completes subtask 1 (pun detection)\n",
    "Outputs the results for heterographic and homographic puns to two seperate files\n",
    "\n",
    "In order to run the scoring system for the heterographic puns:\n",
    "open terminal and change directory to ~/datasets/scoring/bin\n",
    "then, type (your file paths will vary):\n",
    "java de.tudarmstadt.ukp.semeval2017.task7.scorer.PunScorer -d ~/Desktop/GitHub/NLP-Final-Project/datasets/data/test/subtask1-heterographic-test.gold ~/Desktop/GitHub/NLP-Final-Project/system_output/subtask1_heterographic.txt ~/Desktop/GitHub/NLP-Final-Project/scorer_results/subtask1_heterographic_results.txt\n",
    "\n",
    "for homographic puns:\n",
    "open terminal and change directory to ~/datasets/scoring/bin\n",
    "then, type (your file paths will vary): \n",
    "java de.tudarmstadt.ukp.semeval2017.task7.scorer.PunScorer -d ~/Desktop/GitHub/NLP-Final-Project/datasets/data/test/subtask1-homographic-test.gold ~/Desktop/GitHub/NLP-Final-Project/system_output/subtask1_homographic.txt ~/Desktop/GitHub/NLP-Final-Project/scorer_results/subtask1_homographic_results.txt\n",
    "\n",
    "\"\"\"\n",
    "def subtask1():\n",
    "    contains_pun_heterographic = detect_puns('datasets/data/test/subtask1-heterographic-test.xml', True)[0]\n",
    "    with open('system_output/subtask1_heterographic.txt', 'w') as filehandle:\n",
    "        for pun_result in contains_pun_heterographic:\n",
    "            filehandle.write('{}\\n'.format(pun_result))\n",
    "\n",
    "    contains_pun_homographic = detect_puns('datasets/data/test/subtask1-homographic-test.xml', False)[0]\n",
    "    with open('system_output/subtask1_homographic.txt', 'w') as filehandle:\n",
    "        for pun_result in contains_pun_homographic:\n",
    "            filehandle.write('{}\\n'.format(pun_result))\n",
    "            \n",
    "subtask1()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locate_puns(file, heterographic):\n",
    "    \"\"\"\n",
    "    generate pmi scores\n",
    "    the second word in the word pair with the highest pmi score in a given sentence is the pun word\n",
    "    if a given index contains a pun,\n",
    "    check the highest pmi score in the dictionary at that index & find the second word\n",
    "    Append this to the results list\n",
    "    in order to get the correct sentence id for a given pun, I need to find the sentence in the \n",
    "    \"\"\"\n",
    "    print(\"Beginning pun location...\")\n",
    "    \n",
    "    formatted_results = []\n",
    "    stop = stopwords.words('english') + list(string.punctuation)\n",
    "   \n",
    "    # get pmi scores in the form of a dictionary for each sentence mapping each word_pair to a pmi score\n",
    "    ordered_pmi_scores, skipgram, word_dict, sentences_dict = generate_pmi_scores(file)\n",
    "    \n",
    "    # this will be a dict mapping sentence_id to the pun_word_index (starting from 1, not 0) in that sentence\n",
    "    sent2punidx = dict()\n",
    "    sentences = list(sentences_dict.keys())\n",
    "\n",
    "    for i in range(1, len(ordered_pmi_scores)):\n",
    "        if (len(ordered_pmi_scores[i]) == 0):\n",
    "            continue\n",
    "            \n",
    "        curr_dict = ordered_pmi_scores[i]\n",
    "        highest_pmi = list(curr_dict.items())[0][0] # get word_pair in current sentence with highest pmi\n",
    "        pun_index = 0\n",
    "        \n",
    "        # find the sentence_id that this word_pair belongs to\n",
    "        for j in range(len(sentences)):\n",
    "            found_sentence = False\n",
    "            \n",
    "            if highest_pmi[0] and highest_pmi[1] in sentences[j].lower():\n",
    "                found_sentence = True\n",
    "                curr_sent = word_tokenize(sentences[j].lower())\n",
    "                sentence_id = sentences_dict[sentences[j]]\n",
    "              \n",
    "                for word in word_dict[sentence_id]:\n",
    "                    if word[0].string == highest_pmi[1]:\n",
    "                        pun_index = word[1]\n",
    "                        sent2punidx[sentence_id] = pun_index\n",
    "                        break\n",
    "                    \n",
    "            # no need to check the rest of the sentences since we already found where the pun is located\n",
    "            if found_sentence: \n",
    "                break \n",
    "                                \n",
    "    for pun in list(sent2punidx.items()):\n",
    "        formatted_results.append(str(pun[0]) + \" \" + str(pun[1]))\n",
    "    \n",
    "    print(\"Finished locating puns\")\n",
    "\n",
    "    return formatted_results     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning pun location...\n",
      "Finished locating puns\n",
      "Beginning pun location...\n",
      "Finished locating puns\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Completes subtask 2 (pun location)\n",
    "Outputs the results for heterographic and homographic puns to two seperate files\n",
    "\n",
    "In order to run the scoring system for the heterographic puns:\n",
    "open terminal and change directory to ~/datasets/scoring/bin\n",
    "then, enter (your file paths will vary): \n",
    "java de.tudarmstadt.ukp.semeval2017.task7.scorer.PunScorer -l ~/Desktop/GitHub/NLP-Final-Project/datasets/data/test/subtask2-heterographic-test.gold ~/Desktop/GitHub/NLP-Final-Project/system_output/subtask2_heterographic.txt ~/Desktop/GitHub/NLP-Final-Project/scorer_results/subtask2_heterographic_results.txt\n",
    "\n",
    "for homographic puns:\n",
    "open terminal and change directory to ~/datasets/scoring/bin\n",
    "then, enter (your file paths will vary): \n",
    "java de.tudarmstadt.ukp.semeval2017.task7.scorer.PunScorer -l ~/Desktop/GitHub/NLP-Final-Project/datasets/data/test/subtask2-homographic-test.gold ~/Desktop/GitHub/NLP-Final-Project/system_output/subtask2_homographic.txt ~/Desktop/GitHub/NLP-Final-Project/scorer_results/subtask2_homographic_results.txt\n",
    "\"\"\"\n",
    "def subtask2():\n",
    "    locate_pun_heterographic = locate_puns('datasets/data/test/subtask2-heterographic-test.xml', True)\n",
    "    with open('system_output/subtask2_heterographic.txt', 'w') as filehandle:\n",
    "        for pun_location in locate_pun_heterographic:\n",
    "            filehandle.write('{}\\n'.format(pun_location))\n",
    "\n",
    "    locate_pun_homographic = locate_puns('datasets/data/test/subtask2-homographic-test.xml', False)\n",
    "    with open('system_output/subtask2_homographic.txt', 'w') as filehandle:\n",
    "        for pun_location in locate_pun_homographic:\n",
    "            filehandle.write('{}\\n'.format(pun_location))\n",
    "            \n",
    "subtask2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to run the code below, you will need to download the dataset here http://magnitude.plasticity.ai/word2vec/medium/GoogleNews-vectors-negative300.magnitude\n",
    "# and place it in the directory with this .ipynb file\n",
    "wordvectors = Magnitude('wiki-news-300d-1M.magnitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpret_puns(file):\n",
    "    \"\"\" \n",
    "    finds the two senses (based on WordNet senses) of the given pun word\n",
    "    First sense is found using pywsd simple lesk algorithm\n",
    "    The second sense of the word is the synonym of the \n",
    "    \"\"\"\n",
    "    print(\"Beginning pun interpretation...\")\n",
    "    ordered_pmi_scores, skipgram, word_dict, sentences_dict = generate_pmi_scores(file)\n",
    "    \n",
    "    #print(sentences_dict)\n",
    "    # create a mapping of sentence (e.g., \"het_1\") to pun location and word (e.g., (\"het_1_14\", \"allegedly\") )\n",
    "    puns = dict()\n",
    "    for sent in list(word_dict.items()):\n",
    "        for word2id in sent[1]:\n",
    "            if word2id[0]['senses'] == '2':\n",
    "                puns[sent[0]] = (word2id[0]['id'], word2id[0].string)\n",
    "                \n",
    "    first_senses = dict()\n",
    "    \n",
    "    # find first sense of word using pywsd\n",
    "    for pun in list(puns.items()):\n",
    "        curr_sent = \"\"\n",
    "        curr_key = \"\"\n",
    "        for sent in list(sentences_dict.items()):\n",
    "            # find the full sentence that the given pun belongs to\n",
    "            if sent[1] == pun[0]:\n",
    "                curr_key = sent[1]\n",
    "                curr_sent = sent[0]\n",
    "                break\n",
    "        \n",
    "        curr_pun = pun[1][1]\n",
    "        sense = simple_lesk(curr_sent, curr_pun)\n",
    "        if sense is not None:\n",
    "            sense_key = sense.lemmas()[0].key()\n",
    "            first_senses[pun[1][0]] = sense_key\n",
    "            \n",
    "    punloc2wdpair = dict()\n",
    "    pun_list = list(puns.items())\n",
    "    second_senses = dict()\n",
    "\n",
    "    # get the pmi_word pair associated with the pun word in each sentence\n",
    "    for i in range(1, len(ordered_pmi_scores)):\n",
    "        curr_sent_pmi = list(ordered_pmi_scores[i].items())\n",
    "        if len(curr_sent_pmi) != 0:\n",
    "            punloc2wdpair[pun_list[i-1][1][0]] = curr_sent_pmi[0]\n",
    "        \n",
    "\n",
    "    for pun in list(puns.items()):\n",
    "        if pun[1][0] in list(punloc2wdpair.keys()):\n",
    "            curr_pmi_pair = punloc2wdpair[pun[1][0]]\n",
    "            non_pun_word = curr_pmi_pair[0][0]\n",
    "            pun_word = curr_pmi_pair[0][1]\n",
    "            syns = wordnet.synset(pun_word) # get all synonyms of pun word\n",
    "            # compare all of the pun's synonyms with the non pun word to get the second sense\n",
    "            most_similar = wordVectors.most_similar_to_given(non_pun_word, syns.lemma_names)\n",
    "            second_senses[pun[1][0]] = most_similar\n",
    "    \n",
    "    \n",
    "    formatted_results = []\n",
    "    for pun in list(puns.items()):\n",
    "        pun_loc = pun[1][0]\n",
    "        # if the pun word's senses were successfully found, then add them to the final results\n",
    "        if pun_loc in list(first_senses.keys()): #and pun_loc in list(second_senses.keys()):\n",
    "            curr_line = str(pun_loc) + ' ' + str(first_senses[pun_loc]) # + ' ' + str(second_senses[pun_loc])\n",
    "            formatted_results.append(curr_line)\n",
    "            \n",
    "    print(\"Finished pun interpretation\")                               \n",
    "    return formatted_results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning pun interpretation...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-9faffd890ab7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mfilehandle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpun_interpretation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0msubtask3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-92-9faffd890ab7>\u001b[0m in \u001b[0;36msubtask3\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubtask3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0minterpret_puns_het\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minterpret_puns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'datasets/data/test/subtask3-heterographic-test.xml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'system_output/subtask3_heterographic.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfilehandle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpun_interpretation\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minterpret_puns_het\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mfilehandle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpun_interpretation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-91-dbc306ff495c>\u001b[0m in \u001b[0;36minterpret_puns\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0mnon_pun_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurr_pmi_pair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0mpun_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurr_pmi_pair\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0msyns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"car\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msyns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/venv/lib/python3.6/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36msynset\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1327\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1328\u001b[0m         \u001b[0;31m# split name into lemma, part of speech and synset number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m         \u001b[0mlemma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset_index_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1330\u001b[0m         \u001b[0msynset_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msynset_index_str\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 1)"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Completes subtask 3 (pun interpretation)\n",
    "Outputs the results for heterographic and homographic puns to two seperate files\n",
    "\n",
    "In order to run the scoring system for the heterographic puns:\n",
    "open terminal and change directory to ~/datasets/scoring/bin\n",
    "then, enter (your file paths will vary): \n",
    "java de.tudarmstadt.ukp.semeval2017.task7.scorer.PunScorer -l ~/Desktop/GitHub/NLP-Final-Project/datasets/data/test/subtask2-heterographic-test.gold ~/Desktop/GitHub/NLP-Final-Project/system_output/subtask2_heterographic.txt ~/Desktop/GitHub/NLP-Final-Project/scorer_results/subtask2_heterographic_results.txt\n",
    "\n",
    "for homographic puns:\n",
    "open terminal and change directory to ~/datasets/scoring/bin\n",
    "then, enter (your file paths will vary): \n",
    "java de.tudarmstadt.ukp.semeval2017.task7.scorer.PunScorer -l ~/Desktop/GitHub/NLP-Final-Project/datasets/data/test/subtask2-homographic-test.gold ~/Desktop/GitHub/NLP-Final-Project/system_output/subtask2_homographic.txt ~/Desktop/GitHub/NLP-Final-Project/scorer_results/subtask2_homographic_results.txt\n",
    "\"\"\"\n",
    "\n",
    "def subtask3():\n",
    "    interpret_puns_het = interpret_puns('datasets/data/test/subtask3-heterographic-test.xml')\n",
    "    with open('system_output/subtask3_heterographic.txt', 'w') as filehandle:\n",
    "        for pun_interpretation in interpret_puns_het:\n",
    "            filehandle.write('{}\\n'.format(pun_interpretation))\n",
    "    \n",
    "    interpret_puns_hom = interpret_puns('datasets/data/test/subtask3-homographic-test.xml')\n",
    "    with open('system_output/subtask3_homographic.txt', 'w') as filehandle:\n",
    "        for pun_interpretation in interpret_puns_hom:\n",
    "            filehandle.write('{}\\n'.format(pun_interpretation))\n",
    "    \n",
    "subtask3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
