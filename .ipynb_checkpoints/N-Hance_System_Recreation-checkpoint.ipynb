{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Names: Jorge Mazariegos & Cameron Knopp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/camknopp/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# imports statements\n",
    "import time\n",
    "import string\n",
    "import itertools\n",
    "import operator\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import nltk\n",
    "from scipy.stats import iqr\n",
    "from statistics import median\n",
    "from collections import defaultdict, OrderedDict, Counter\n",
    "from bs4 import BeautifulSoup\n",
    "#from gensim.models import KeyedVectors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "set(stopwords.words('english'))\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-paper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess should take in the datasets (.xml) and prepare them to be used\n",
    "def preprocess(data):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data (list(str)):\n",
    "    Returns: a list of tokens and a list of tokenized sentences\n",
    "\n",
    "    \"\"\"\n",
    "    #######################################################\n",
    "    # initialize variables to use in preprocess\n",
    "    #######################################################\n",
    "    puns = []\n",
    "    tokens = []\n",
    "    stop = stopwords.words('english')\n",
    "    \n",
    "    #######################################################\n",
    "    # Given a sentence, tokenize it and append it to a list\n",
    "    #######################################################\n",
    "    for sentence in data:\n",
    "        puns.append(word_tokenize(sentence.lower())) # creates the list of all sentences\n",
    "        \n",
    "    #######################################################\n",
    "    # Every sentence is tokenized, but let's grab each\n",
    "    # individual word to make a vocab out of.\n",
    "    #######################################################\n",
    "    for sentence in puns:\n",
    "        for word in sentence:\n",
    "            if(word.isalpha()): # filter out punctuation\n",
    "                tokens.append(word)\n",
    "    #######################################################\n",
    "    # Remove stop words from tokens\n",
    "    #######################################################\n",
    "    tokens_with_stop_words = tokens\n",
    "    tokens = [token for token in tokens_with_stop_words if token not in stop]\n",
    "\n",
    "    return tokens, puns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(file):\n",
    "\n",
    "    # DATA PROCESSING #\n",
    "    #######################################################\n",
    "    # Open the dataset/'s we will be using and process the\n",
    "    # text within to be used by our code.\n",
    "    #######################################################\n",
    "    #f = open('datasets/data/test/subtask1-heterographic-test.xml', 'r', encoding = 'utf8')\n",
    "    \n",
    "    f = open(file, 'r', encoding = 'utf8')\n",
    "    data = f.read()\n",
    "\n",
    "    #######################################################\n",
    "    # Using Beautiful Soup we can easily extract the puns\n",
    "    # from the given datasets.\n",
    "    #######################################################\n",
    "    soup = BeautifulSoup(data, 'xml')\n",
    "    ids = soup.find_all('text')\n",
    "    words = soup.find_all('word')\n",
    "    #######################################################\n",
    "    # Create a list of all sentences within the dataset to hand\n",
    "    # over to our preprocess function\n",
    "    #######################################################\n",
    "    wurd = \"\"\n",
    "    sentence = \"\"\n",
    "    sentences = []\n",
    "    pun_list = []\n",
    "    for i in range(len(ids)):\n",
    "        for line in ids[i]:\n",
    "            for word in line:\n",
    "                if(word != '\\n' or word == '\\''):\n",
    "                    if(word.isalpha()): # If not punctuation\n",
    "                        wurd = word\n",
    "                        if(sentence == \"\"): # If the start of the sentence\n",
    "                            sentence = sentence + wurd\n",
    "                        else: # If not the start of the sentence\n",
    "                            sentence = sentence + \" \" + wurd\n",
    "                    else: # If punctuation we don't want to put a space between the character and it.\n",
    "                        wurd = word\n",
    "                        sentence = sentence + wurd\n",
    "                    wurd = \"\" # clear the current word\n",
    "        sentences.append(sentence) # append the created string sentence to our list.\n",
    "        sentence = \"\"\n",
    "    #######################################################\n",
    "    # Create a list of tokens to make a vocabulary of and\n",
    "    # create a list of sentences to create make word pairs\n",
    "    # from.\n",
    "    #######################################################\n",
    "    \n",
    "    # return token, pun_list\n",
    "    return preprocess(sentences)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, special_tokens=None):\n",
    "        self.w2idx = {}\n",
    "        self.idx2w = {}\n",
    "        self.w2cnt = defaultdict(int)\n",
    "        self.special_tokens = special_tokens\n",
    "        if self.special_tokens is not None:\n",
    "            self.add_tokens(special_tokens)\n",
    "\n",
    "    def add_tokens(self, tokens):\n",
    "        for token in tokens:\n",
    "            self.add_token(token)\n",
    "            self.w2cnt[token] += 1\n",
    "\n",
    "    def add_token(self, token):\n",
    "        if token not in self.w2idx:\n",
    "            cur_len = len(self)\n",
    "            self.w2idx[token] = cur_len\n",
    "            self.idx2w[cur_len] = token\n",
    "\n",
    "    def prune(self, min_cnt=2):\n",
    "        to_remove = set([token for token in self.w2idx if self.w2cnt[token] < min_cnt])\n",
    "        if self.special_tokens is not None:\n",
    "            to_remove = to_remove.difference(set(self.special_tokens))\n",
    "        \n",
    "        for token in to_remove:\n",
    "            self.w2cnt.pop(token)\n",
    "            \n",
    "        self.w2idx = {token: idx for idx, token in enumerate(self.w2cnt.keys())}\n",
    "        self.idx2w = {idx: token for token, idx in self.w2idx.items()}\n",
    "    \n",
    "    def __contains__(self, item):\n",
    "        return item in self.w2idx\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        if isinstance(item, str):\n",
    "            return self.w2idx[item]\n",
    "        elif isinstance(item , int):\n",
    "            return self.idx2w[item]\n",
    "        else:\n",
    "            raise TypeError(\"Supported indices are int and str\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return(len(self.w2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################\n",
    "# Using skipgrams we can create the wordpairs described\n",
    "# in the N-Hance research paper.\n",
    "#######################################################\n",
    "\n",
    "class SkipGramDataset(Dataset):\n",
    "    def __init__(self, data, vocab, skip_window=3):\n",
    "        super().__init__()\n",
    "\n",
    "        #######################################################\n",
    "        # Unlike before, data will be a list of strings handed\n",
    "        # all at once.\n",
    "        #######################################################\n",
    "        self.vocab = vocab\n",
    "        self.data = data\n",
    "        # set skip_window to the length of the longest sentence in the data set\n",
    "        self.skip_window =  max(data, key=len)\n",
    "        self.pairs = self._generate_pairs(data, skip_window)\n",
    "        \n",
    "        \n",
    "    #######################################################\n",
    "    # generate word pairs given list of lists of words representing each sentence\n",
    "    #######################################################\n",
    "    def _generate_pairs(self, data, skip_window):\n",
    "\n",
    "        pairs = [[]]  # list of word pairs for each sentence\n",
    "        curr_sentence_pairs = [] # list of word pairs for current sentence\n",
    "        pruned_pairs = []\n",
    "        \n",
    "\n",
    "        for sent in data: \n",
    "            for i in range(len(sent)):\n",
    "                for j in range(-skip_window, skip_window + 1):\n",
    "                    context_idx = i + j\n",
    "                    if j == 0 or context_idx < 0 or context_idx >= len(sent):\n",
    "                        continue\n",
    "                    if sent[i] not in self.vocab or sent[context_idx] not in self.vocab:\n",
    "                        continue\n",
    "                        \n",
    "                    # only add in this sentence if the reverse does not already exist in the list\n",
    "                    if (sent[context_idx], sent[i]) not in curr_sentence_pairs:\n",
    "                        curr_sentence_pairs.append((sent[i], sent[context_idx]))\n",
    "                    \n",
    "            pairs.append(curr_sentence_pairs.copy()) # need to append a copy so that it is not cleared with we call clear() in the next line\n",
    "            curr_sentence_pairs.clear()\n",
    "                    \n",
    "        return pairs\n",
    "    \n",
    "    #######################################################\n",
    "    # returns the list of word_pairs for the sentence at the given index\n",
    "    #######################################################\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        pair = self.pairs[idx]\n",
    "\n",
    "        #pair = [self.vocab[t] for t in pair]\n",
    "        #pair = [self.vocab.__getitem__(t) for t in pair]\n",
    "        return pair\n",
    "    \n",
    "    #######################################################\n",
    "    # returns the number of sentences\n",
    "    #######################################################\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pmi_scores(file):\n",
    "    \"\"\"\n",
    "    returns a list of dictionaries (one for each sentence) of {word_pair : pmi_score}\n",
    "    each dictionary is ordered from highest to lowest pmi score\n",
    "    \"\"\"\n",
    "    \n",
    "    # Tokenize dataset and Create a Vocabulary using the tokens\n",
    "    tokens, pun_list = data_process(file)\n",
    "    voc = Vocabulary()\n",
    "    voc.add_tokens(tokens)\n",
    "    \n",
    "    # create skipgram model using vocab and puns\n",
    "    skipgram = SkipGramDataset(pun_list, voc, skip_window=2)\n",
    "    \n",
    "    # create a Counter object to get counts of individual words\n",
    "    all_sentences = list(itertools.chain.from_iterable(skipgram.data.copy())) \n",
    "    word_counts = Counter(all_sentences)\n",
    "    total_words = len(all_sentences)\n",
    "    \n",
    "    # get list of lists of word_pairs for each sentence\n",
    "    word_pairs = skipgram.pairs.copy()\n",
    "    word_pairs = [[(a,b) for (a,b) in sent] for sent in word_pairs] \n",
    "   # print(word_pairs)\n",
    "        \n",
    "    # create Counter object to get counts for each word_pair\n",
    "    all_word_pairs= list(itertools.chain.from_iterable(word_pairs.copy())) # join all sentences together\n",
    "    all_word_pairs = [(a,b) for (a,b) in all_word_pairs] \n",
    "    total_word_pairs = len(all_word_pairs)\n",
    "    word_pair_count = Counter(all_word_pairs)\n",
    "    \n",
    "    # create a list of dictionaries for each sentence { word_pair : pmi_score }\n",
    "    pmi_scores = list(dict())\n",
    "    current_pmi = 0\n",
    "    current_dict = {}\n",
    "    \n",
    "    # now we will calculate the PMI score for each word_pair\n",
    "    # the formula for PMI score is: log[p(x,y) / (p(x)*p(y))]\n",
    "    \n",
    "    for i in range(skipgram.__len__()):\n",
    "        \n",
    "        # for each sentence, find pmi score for each individual word_pair\n",
    "        for w_pair in word_pairs[i]:\n",
    "            numerator = word_pair_count[w_pair] / total_word_pairs\n",
    "            denominator = (word_counts[w_pair[0]] / total_words) * (word_counts[w_pair[1]] / total_words)\n",
    "            current_pmi =  numerator / denominator\n",
    "            current_pmi = math.log(current_pmi)\n",
    "        \n",
    "            current_dict.update({w_pair : current_pmi}) # add bigram's pmi score to dictionary at index i (the current sentence)\n",
    "        \n",
    "        pmi_scores.append(current_dict.copy())\n",
    "        current_dict.clear()\n",
    "        \n",
    "    \n",
    "    # now we sort the dictionary entries from highest->lowest based on value (PMI score)\n",
    "    ordered_pmi_scores = list(OrderedDict())\n",
    "    \n",
    "    for i in range(len(pmi_scores)):\n",
    "        current_dict = pmi_scores[i]\n",
    "        # convert to dictionary ordered by value (which is the pmi score in this case)\n",
    "        current_ordered_dict = OrderedDict(sorted(current_dict.items(), key=lambda x: x[1], reverse=True))\n",
    "        ordered_pmi_scores.append(current_ordered_dict.copy())\n",
    "        current_ordered_dict.clear()\n",
    "        \n",
    "    return ordered_pmi_scores\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_puns(file, heterographic):\n",
    "    \"\"\"\n",
    "    create word_pairs for sentences in given file\n",
    "    calculate pmi scores for all given word_pairs\n",
    "    calculate the interquartile range for the pmi scores of word_pairs in each sentence\n",
    "    find the median value of the interquartile ranges across all sentences in the given dataset\n",
    "    for each sentence, if the highest pmi score - second highest pmi score > median interquartile range ...\n",
    "    (cont.) then that means that that sentence contains a pun\n",
    "    \"\"\"\n",
    "    \n",
    "    # homographic pun 5 would be referred to as hom5 in the final list (this is based on the N-Hance system's guidelines)\n",
    "    if heterographic:\n",
    "        prefix = \"het\"\n",
    "    else:\n",
    "        prefix = \"hom\"\n",
    "    \n",
    "    # get pmi scores for all word_pairs in the file\n",
    "    ordered_pmi_scores = generate_pmi_scores(file)\n",
    "    \n",
    "    # now we need to find the interquartile range for each dictionary in the list using iqr from scipy.stats\n",
    "    iqr_values = []\n",
    "    \n",
    "    for dictionary in ordered_pmi_scores:\n",
    "        iqr_values.append(iqr(list(dictionary.values())))\n",
    "    \n",
    "    \n",
    "    # now we take the median of these iqr values and take that as our iqr value of the current dataset\n",
    "    median_iqr = median(iqr_values)\n",
    "    \n",
    "    # create a list which will contain True (yes, this sentence contains a pun) or False (no, this sentence does not contain a pun)\n",
    "    # ... at each index\n",
    "    contains_pun = []\n",
    "    \n",
    "    for i in range(1, len(ordered_pmi_scores)):\n",
    "        curr_dict = list(ordered_pmi_scores[i].items())\n",
    "\n",
    "        if len(curr_dict) > 1 :\n",
    "            # if the difference between the highest pmi score and second highest pmi score (cont.)\n",
    "            #... is greater than the median iqr, then the sentence contains a pun\n",
    "            if float(curr_dict[0][1] - curr_dict[1][1]) > median_iqr:\n",
    "                contains_pun.append(prefix + \"_\" + str(i) + \" 1\" )\n",
    "                \n",
    "            else:\n",
    "                contains_pun.append(prefix + \"_\" + str(i) + \" 0\" )\n",
    "        else:\n",
    "            contains_pun.append(prefix + \"_\" + str(i) + \" 0\" )\n",
    "        \n",
    "\n",
    "    return contains_pun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Completes subtask 1 (pun detection)\n",
    "Outputs the results for heterographic and homographic puns to two seperate files\n",
    "\n",
    "In order to run the scoring system for the heterographic puns:\n",
    "first, change directory to scoring/bin\n",
    "then, type (your file paths will vary): java de.tudarmstadt.ukp.semeval2017.task7.scorer.PunScorer -d ~/Desktop/GitHub/NLP-Final-Project/datasets/data/test/subtask1-heterographic-test.gold ~/Desktop/GitHub/NLP-Final-Project/subtask1_heterographic.txt\n",
    "\n",
    "for homographic puns:\n",
    "first, change directory to scoring/bin\n",
    "then, type (your file paths will vary): java de.tudarmstadt.ukp.semeval2017.task7.scorer.PunScorer -d ~/Desktop/GitHub/NLP-Final-Project/datasets/data/test/subtask1-homographic-test.gold ~/Desktop/GitHub/NLP-Final-Project/subtask1_homographic.txt\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "contains_pun_heterographic = detect_puns('datasets/data/test/subtask1-heterographic-test.xml', True)\n",
    "with open('subtask1_heterographic.txt', 'w') as filehandle:\n",
    "    for pun_result in contains_pun_heterographic:\n",
    "        filehandle.write('{}\\n'.format(pun_result))\n",
    "\n",
    "contains_pun_homographic = detect_puns('datasets/data/test/subtask1-homographic-test.xml', False)\n",
    "with open('subtask1_homographic.txt', 'w') as filehandle:\n",
    "    for pun_result in contains_pun_homographic:\n",
    "        filehandle.write('{}\\n'.format(pun_result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locate_pun(file, heterographic):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class SkipGramModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size (int): vocabulary size\n",
    "            embedding_dim (int): the dimension of word embeddings\n",
    "        \"\"\"\n",
    "        ### INSERT YOUR CODE BELOW ###\n",
    "        #self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        #self.linear = torch.nn.Linear(1, vocab_size)\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = torch.nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "        ### INSERT YOUR CODE ABOVE ###\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Perform the forward pass of the skip-gram model.\n",
    "        \n",
    "        Args:\n",
    "            inputs (torch.LongTensor): input tensor containing batches of word ids [Bx1]\n",
    "        Returns:\n",
    "            outputs (torch.FloatTensor): output tensor with unnormalized probabilities over the vocabulary [BxV]\n",
    "        \"\"\"\n",
    "        ### INSERT YOUR CODE BELOW ###\n",
    "        embeds = self.embedding(inputs)\n",
    "        #embeds = self.embedding(inputs)\n",
    "        outputs = self.linear(embeds)\n",
    "        outputs=outputs\n",
    "        #output = F.log_softmax(self.linear(embeds), dim=1)\n",
    "        ### INSERT YOUR CODE ABOVE ###\n",
    "        return outputs\n",
    "    \n",
    "    def save_embeddings(self, voc, path):\n",
    "        \"\"\"\n",
    "        Save the embedding matrix to a specified path.\n",
    "        \n",
    "        Args:\n",
    "            voc (Vocabulary): the Vocabulary object for id-to-token mapping\n",
    "            path (str): the location of the target file\n",
    "        \"\"\"\n",
    "        ### INSERT YOUR CODE BELOW ###\n",
    "        embeds = self.embedding.weight.data.cpu().numpy()\n",
    "        f = open(path, 'w')\n",
    "        f.write(str(vocab_size) + ' ' + str(embedding_dim) + '\\n')\n",
    "        \n",
    "        for idx in range(len(embeds)):\n",
    "            word = voc.idx2w[idx]\n",
    "            embedding = ' '.join(map(str,embeds[idx]))\n",
    "            f.write(word + ' '+ embedding + '\\n')\n",
    "        ### INSERT YOUR CODE ABOVE ###\n",
    "        print(\"Successfuly saved to {}\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
