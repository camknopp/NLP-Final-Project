{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Names: Jorge Mazariegos & Cameron Knopp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/camknopp/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# imports statements\n",
    "import time\n",
    "from collections import defaultdict, Counter\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "#from gensim.models import KeyedVectors\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "set(stopwords.words('english'))\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-paper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess should take in the datasets (.xml) and prepare them to be used\n",
    "def preprocess(data):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data (str):\n",
    "    Returns: a list of tokens\n",
    "\n",
    "    \"\"\"\n",
    "    ### YOUR CODE BELOW ###\n",
    "    tokens = []\n",
    "    tokens_with_punct = []\n",
    "    for sentence in data:\n",
    "        for word in sentence:\n",
    "             tokens_with_punct.append(word.lower())\n",
    "    for temp in tokens_with_punct:\n",
    "        for word in temp:\n",
    "            tokens.append((word_tokenize(word)))\n",
    "    stop = stopwords.words('english')\n",
    "    tokens = [token for token in tokens_with_punct if token.isalpha()] # remove punctuation from tokens\n",
    "    tokens_with_stop_words = tokens\n",
    "    tokens = [token for token in tokens_with_stop_words if token not in stop]\n",
    "    ### YOUR CODE ABOVE ###\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA PROCESSING #\n",
    "f = open('datasets/data/test/subtask1-heterographic-test.xml', 'r', encoding = 'utf8')\n",
    "data = f.read()\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(data, 'xml')\n",
    "ids = soup.find_all('text')\n",
    "words = soup.find_all('word')\n",
    "\n",
    "sentence = []\n",
    "punList = []\n",
    "for i in range(0, len(ids)):\n",
    "    for line in ids[i]:\n",
    "        for word in line:\n",
    "            if(word != '\\n'):\n",
    "                sentence.append(word)       \n",
    "    punList.append(sentence)\n",
    "    sentence = []\n",
    "tokens = preprocess(punList)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, special_tokens=None):\n",
    "        self.w2idx = {}\n",
    "        self.idx2w = {}\n",
    "        self.w2cnt = defaultdict(int)\n",
    "        self.special_tokens = special_tokens\n",
    "        if self.special_tokens is not None:\n",
    "            self.add_tokens(special_tokens)\n",
    "\n",
    "    def add_tokens(self, tokens):\n",
    "        for token in tokens:\n",
    "            self.add_token(token)\n",
    "            self.w2cnt[token] += 1\n",
    "\n",
    "    def add_token(self, token):\n",
    "        if token not in self.w2idx:\n",
    "            cur_len = len(self)\n",
    "            self.w2idx[token] = cur_len\n",
    "            self.idx2w[cur_len] = token\n",
    "\n",
    "    def prune(self, min_cnt=2):\n",
    "        to_remove = set([token for token in self.w2idx if self.w2cnt[token] < min_cnt])\n",
    "        if self.special_tokens is not None:\n",
    "            to_remove = to_remove.difference(set(self.special_tokens))\n",
    "        \n",
    "        for token in to_remove:\n",
    "            self.w2cnt.pop(token)\n",
    "            \n",
    "        self.w2idx = {token: idx for idx, token in enumerate(self.w2cnt.keys())}\n",
    "        self.idx2w = {idx: token for token, idx in self.w2idx.items()}\n",
    "    \n",
    "    def __contains__(self, item):\n",
    "        return item in self.w2idx\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        if isinstance(item, str):\n",
    "            return self.w2idx[item]\n",
    "        elif isinstance(item , int):\n",
    "            return self.idx2w[item]\n",
    "        else:\n",
    "            raise TypeError(\"Supported indices are int and str\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return(len(self.w2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramDataset(Dataset):\n",
    "    def __init__(self, data, vocab, skip_window=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.data = data\n",
    "        self.skip_window = skip_window\n",
    "\n",
    "        self.pairs = self._generate_pairs(data, skip_window)\n",
    "\n",
    "    def _generate_pairs(self, data, skip_window):\n",
    "        \"\"\"\n",
    "        Args: input data (a list of tokens)\n",
    "        Returns: all possible pairs for the SkipGram mode\n",
    "        \"\"\"\n",
    "        pairs = []\n",
    "\n",
    "        for i in range(len(data)):\n",
    "            for j in range(-skip_window, skip_window + 1):\n",
    "                context_idx = i + j\n",
    "                if j == 0 or context_idx < 0 or context_idx >= len(data):\n",
    "                    continue\n",
    "                if data[i] not in self.vocab or data[context_idx] not in self.vocab:\n",
    "                    continue\n",
    "                pairs.append((data[i], data[context_idx]))\n",
    "        return pairs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            idx\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        pair = self.pairs[idx]\n",
    "        pair = [self.vocab[t] for t in pair]\n",
    "        return pair\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        \"\"\"\n",
    "        return len(self.pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class SkipGramModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size (int): vocabulary size\n",
    "            embedding_dim (int): the dimension of word embeddings\n",
    "        \"\"\"\n",
    "        ### INSERT YOUR CODE BELOW ###\n",
    "        #self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        #self.linear = torch.nn.Linear(1, vocab_size)\n",
    "        self.embedding = torch.nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear = torch.nn.Linear(embedding_dim, vocab_size)\n",
    "        \n",
    "        ### INSERT YOUR CODE ABOVE ###\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Perform the forward pass of the skip-gram model.\n",
    "        \n",
    "        Args:\n",
    "            inputs (torch.LongTensor): input tensor containing batches of word ids [Bx1]\n",
    "        Returns:\n",
    "            outputs (torch.FloatTensor): output tensor with unnormalized probabilities over the vocabulary [BxV]\n",
    "        \"\"\"\n",
    "        ### INSERT YOUR CODE BELOW ###\n",
    "        embeds = self.embedding(inputs)\n",
    "        #embeds = self.embedding(inputs)\n",
    "        outputs = self.linear(embeds)\n",
    "        outputs=outputs\n",
    "        #output = F.log_softmax(self.linear(embeds), dim=1)\n",
    "        ### INSERT YOUR CODE ABOVE ###\n",
    "        return outputs\n",
    "    \n",
    "    def save_embeddings(self, voc, path):\n",
    "        \"\"\"\n",
    "        Save the embedding matrix to a specified path.\n",
    "        \n",
    "        Args:\n",
    "            voc (Vocabulary): the Vocabulary object for id-to-token mapping\n",
    "            path (str): the location of the target file\n",
    "        \"\"\"\n",
    "        ### INSERT YOUR CODE BELOW ###\n",
    "        embeds = self.embedding.weight.data.cpu().numpy()\n",
    "        f = open(path, 'w')\n",
    "        f.write(str(vocab_size) + ' ' + str(embedding_dim) + '\\n')\n",
    "        \n",
    "        for idx in range(len(embeds)):\n",
    "            word = voc.idx2w[idx]\n",
    "            embedding = ' '.join(map(str,embeds[idx]))\n",
    "            f.write(word + ' '+ embedding + '\\n')\n",
    "        ### INSERT YOUR CODE ABOVE ###\n",
    "        print(\"Successfuly saved to {}\".format(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'text8.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-d9bea7e5e79a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# DATA PROCESSING #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'text8.txt'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'text8.txt'"
     ]
    }
   ],
   "source": [
    "# DATA PROCESSING #\n",
    "with open('text8.txt') as f:\n",
    "    data = f.read()\n",
    "tokens = preprocess(data[:1000000])\n",
    "\n",
    "# CONSTRUCTING VOCABULARY #\n",
    "voc = Vocabulary()\n",
    "voc.add_tokens(tokens)\n",
    "voc.prune(5)\n",
    "vocab_size = len(voc)\n",
    "\n",
    "# TRAINING PARAMETERS #\n",
    "embedding_dim = 128\n",
    "skip_window = 2\n",
    "batch_size = 512\n",
    "lr = 0.1\n",
    "num_epochs = 100\n",
    "report_every = 1\n",
    "\n",
    "# DATASET\n",
    "dataset = SkipGramDataset(tokens, voc, skip_window=skip_window)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# MODEL\n",
    "model = SkipGramModel(vocab_size=vocab_size, embedding_dim=embedding_dim)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING #\n",
    "tick = time.time()\n",
    "epoch_losses = []\n",
    "for epoch_num in range(1, num_epochs + 1):\n",
    "    batch_losses = []\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        ### YOUR CODE BELOW ###\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Extract the inputs and the targets\n",
    "        inputs, targets = batch\n",
    "        \n",
    "        # Transfer the inputs and the targets to GPUs, if available\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = inputs.cuda()\n",
    "            targets = targets.cuda()\n",
    "            \n",
    "        # Run the model\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs,targets)\n",
    "        \n",
    "        # Backpropagate the error\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Append the loss\n",
    "        batch_losses.append(float(loss))\n",
    "        \n",
    "        ### YOUR CODE ABOVE ###\n",
    "        \n",
    "    epoch_loss = np.mean(np.array(batch_losses))\n",
    "    epoch_losses.append(epoch_loss)\n",
    "\n",
    "    if epoch_num % report_every == 0:\n",
    "        tock = time.time()\n",
    "        print(\"Epoch {}. Loss {:.4f}. Elapsed {:.0f} seconds\".format(epoch_num, epoch_loss, tock-tick))\n",
    "\n",
    "print(\"Total time elapsed: {:.0f} minutes\".format((tock-tick)/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curve\n",
    "### YOUR CODE BELOW ###\n",
    "\n",
    "# this normally will work, but it didn't run this time because I exited out after retraining the model and\n",
    "# ...saving the embeddings. However, I didn't run this cell before exiting out, so in order for this cell to run, I\n",
    "#.. have to retrain the dataset, which I don't have the time to do currently. But anyways, it shows a graph that is steadily\n",
    "# .. decreasing over time\n",
    "\n",
    "plt.plot(epoch_losses)\n",
    "\n",
    "### YOUR CODE ABOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
